{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 7: \"Методы оптимизации\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО: Усцов Артем Алексеевич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:54:43.094929Z",
     "start_time": "2022-10-06T18:54:43.085593Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import NLLLoss, Sequential, Linear, Sigmoid, ELU, Tanh, L1Loss, Module, Parameter\n",
    "from torch.autograd import Variable\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.colors import LogNorm\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре мы попробуем сравнить различные методы оптимизации: GD, Momentum, NAG, Adagrad, RMSProp, Adadelta, Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Реализация методов\n",
    "<i> Реализуйте методы GD, Momentum, NAG, RMSProp, Adagrad, Adadelta, Adam.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезная функция: __plt.contour__  \n",
    "Для всех экспериментов подберите параметры так, чтобы метод сошелся к ближайшему локальному минимуму.  \n",
    "Все методы следует запускать из одной и той же точки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:32.831996Z",
     "start_time": "2022-10-06T18:02:32.827971Z"
    }
   },
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = list(parameters)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters:\n",
    "            if parameter.grad is not None:\n",
    "                parameter.grad.detach_()\n",
    "                parameter.grad.zero_()\n",
    "    \n",
    "    def pre_step(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        pass\n",
    "    \n",
    "    def update_param(self, parameter):\n",
    "        raise NotImplementedError()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:32.851657Z",
     "start_time": "2022-10-06T18:02:32.843935Z"
    }
   },
   "source": [
    "def optimize_function(fn, optim, optim_args, start_point, num_iter=50):\n",
    "    weigths = nn.Parameter(torch.FloatTensor(start_point), requires_grad=True)\n",
    "\n",
    "    optim = optim(parameters=[weigths], **optim_args)\n",
    "    points = []\n",
    "    losses = []\n",
    "    for i in range(num_iter):\n",
    "        if hasattr(optim, 'pre_step'):\n",
    "            optim.pre_step()\n",
    "        loss = fn(weigths[0], weigths[1])\n",
    "        points.append(weigths.data.detach().clone())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    points = torch.stack(points, axis=0)\n",
    "    losses = torch.FloatTensor(losses)\n",
    "    return points, losses\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:32.862441Z",
     "start_time": "2022-10-06T18:02:32.853117Z"
    },
    "code_folding": []
   },
   "source": [
    "def compare_optimizers(\n",
    "    fn,\n",
    "    optim_list,\n",
    "    start_point,\n",
    "    x_range=(-5, 5),\n",
    "    y_range=(-5, 5),\n",
    "    xstep=0.2,\n",
    "    ystep=0.2,\n",
    "    minima=None,\n",
    "    num_iter = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw level lines with optimizer behaviour\n",
    "    \"\"\"\n",
    "    xmin, xmax = x_range\n",
    "    ymin, ymax = y_range\n",
    "    x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "    z = fn(torch.from_numpy(x), torch.from_numpy(y))\n",
    "    z = z.detach().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "    if minima:\n",
    "        ax.plot(*minima, 'r*', markersize=18)\n",
    "\n",
    "    fig.suptitle(\"Level lines of optimezed function\")\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "\n",
    "    ax.set_xlim((xmin, xmax))\n",
    "    ax.set_ylim((ymin, ymax))\n",
    "    \n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(optim_list)))\n",
    "    name_losses = {}\n",
    "    \n",
    "    for c, (name, optim, args) in zip(colors, optim_list):\n",
    "        points, losses = optimize_function(fn, optim, args, start_point, num_iter)\n",
    "        ax.quiver(\n",
    "            points[:-1, 0], points[:-1, 1],  \n",
    "            points[1:, 0] - points[:-1, 0], points[1:, 1] - points[:-1, 1], \n",
    "            scale_units='xy', angles='xy', scale=1, color=c,\n",
    "            label=name\n",
    "        )\n",
    "        name_losses[name] = losses\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Loss behaviour in learning\")\n",
    "    plt.xlabel(\"Num of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    x = np.arange(0, num_iter)\n",
    "    for name, losses in name_losses.items():\n",
    "        plt.plot(x, losses.numpy(), label=name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "grad &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "w &=& w - \\eta \\times grad\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:32.885108Z",
     "start_time": "2022-10-06T18:02:32.881250Z"
    }
   },
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad(): # игнорируется для расчета градиентов\n",
    "            for param in self.parameters:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                param -= self.learning_rate * param.grad"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum SGD\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "grad &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "v &=& \\gamma * v + \\eta \\times grad \\\\\n",
    "w &=& w - v\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:33.005767Z",
     "start_time": "2022-10-06T18:02:33.001006Z"
    }
   },
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.V = [torch.zeros_like(param) for param in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for v, param in zip(self.V, self.parameters):\n",
    "                v.copy_(self.gamma * v + self.learning_rate * param.grad)\n",
    "                param -= v"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T16:41:22.762180Z",
     "start_time": "2022-10-03T16:41:22.757952Z"
    }
   },
   "source": [
    "#### Nesterov Adaptive Gradient (NAG)\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "grad &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w_{t}), y_{i}) \\\\\n",
    "w &=& w + v \\\\\n",
    "v &=& \\gamma * v + \\eta \\times grad \\\\\n",
    "w &=& w - v\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:39:30.966920Z",
     "start_time": "2022-10-06T20:39:30.961324Z"
    }
   },
   "source": [
    "class NAG(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.V = [torch.zeros_like(param) for param in self.parameters]\n",
    "        \n",
    "    def step(self): \n",
    "        with torch.no_grad():\n",
    "            for v, param in zip(self.V, self.parameters):\n",
    "                param -= v\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for v, param in zip(self.V, self.parameters):\n",
    "                param += v\n",
    "                v.copy_(self.gamma * v + self.learning_rate * param.grad)\n",
    "                param -= v"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "grad &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "G &=& \\gamma G + (1 - \\gamma) diag(grad * grad^{T}) \\\\\n",
    "w &=& w - \\frac{\\eta}{\\sqrt{G+eps}} \\odot grad\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:33.762745Z",
     "start_time": "2022-10-06T18:02:33.757782Z"
    }
   },
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.G = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.eps = 10e-8\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for g, param in zip(self.G, self.parameters):\n",
    "                g.copy_(self.gamma * g + (1 - self.gamma) * param.grad**2)\n",
    "                param -= self.learning_rate / torch.sqrt(g + self.eps) * param.grad"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T16:43:35.857826Z",
     "start_time": "2022-10-03T16:43:35.854834Z"
    }
   },
   "source": [
    "#### AdaGrad\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "grad &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "G &=& G + diag(grad * grad^{T}) \\\\\n",
    "w &=& w - \\frac{\\eta}{\\sqrt{G+eps}} \\odot grad\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:34.151841Z",
     "start_time": "2022-10-06T18:02:34.146685Z"
    }
   },
   "source": [
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 10e-8\n",
    "        self.G = [torch.zeros_like(param) for param in self.parameters]\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for g, param in zip(self.G, self.parameters):\n",
    "                g.copy_(g + param.grad**2)\n",
    "                param -= self.learning_rate / torch.sqrt(g + self.eps) * param.grad"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T16:43:46.824900Z",
     "start_time": "2022-10-03T16:43:46.816782Z"
    }
   },
   "source": [
    "#### Adadelta\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "grad &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "G &=& G + diag(grad* grad^{T}) \\\\\n",
    "grad' &=& \\frac{\\sqrt{\\Delta w+eps}}{\\sqrt{G+eps}} \\odot grad \\\\\n",
    "\\Delta w &=& \\gamma \\Delta w + (1 - \\rho) * diag(grad'* grad'^{T}) \\\\\n",
    "w &=& w - grad'\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:34.518331Z",
     "start_time": "2022-10-06T18:02:34.512661Z"
    }
   },
   "source": [
    "class Adadelta(Optimizer):\n",
    "    def __init__(self, parameters, gamma=0.9):\n",
    "        super().__init__(parameters)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps = 10e-8\n",
    "        self.G = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.D_Theta = [torch.zeros_like(param) for param in self.parameters]\n",
    "        \n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for g, d_theta, param in zip(self.G, self.D_Theta, self.parameters):\n",
    "                g.copy_(self.gamma * g + (1 - self.gamma) * param.grad**2)\n",
    "                param_hat = (torch.sqrt(d_theta + self.eps) / torch.sqrt(g + self.eps)) * param.grad\n",
    "                d_theta.copy_(self.gamma * d_theta + (1 - self.gamma) * param_hat**2)\n",
    "                param -= param_hat"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "g &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "m &=& \\beta_1 m + (1 - \\beta_1) g \\\\\n",
    "v &=& \\beta_2 v + (1 - \\beta_2) diag(gg^{T}) \\\\\n",
    "\\hat{m} &=& \\frac{m}{1 - \\beta_1} \\\\\n",
    "\\hat{v} &=& \\frac{v}{1 - \\beta_2} \\\\\n",
    "w &=& w - \\frac{\\eta}{\\sqrt{\\hat{v} + \\epsilon}} \\odot \\hat{m}\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:34.894513Z",
     "start_time": "2022-10-06T18:02:34.888350Z"
    }
   },
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta_1=0.9, beta_2=0.99):\n",
    "        super().__init__(parameters)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.eps = 10e-8\n",
    "\n",
    "        self.M = [torch.zeros_like(param) for param in self.parameters]\n",
    "        self.V = [torch.zeros_like(param) for param in self.parameters]\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for m, v, param in zip(self.M, self.V, self.parameters):\n",
    "                m.copy_(self.beta_1 * m + (1 - self.beta_1) * param.grad)\n",
    "                v.copy_(self.beta_2 * v + (1 - self.beta_2) * param.grad**2)\n",
    "                m_hat = m / (1 - self.beta_1)\n",
    "                v_hat = v / (1 - self.beta_2)\n",
    "                param -= self.learning_rate / torch.sqrt(v_hat + self.eps) * m_hat"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравните эти методы на функции $J(x, y) = x^2+y^2$</i>\n",
    "__Взята функция__ $J(x, y) = x^2+y^4$ __для большей наглядности, т.к предложенная слишком быстро сходится__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:35.299782Z",
     "start_time": "2022-10-06T18:02:35.297244Z"
    }
   },
   "source": [
    "def F1(x, y):\n",
    "    return x**2 + y**4"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:39:35.107391Z",
     "start_time": "2022-10-06T20:39:34.049022Z"
    },
    "scrolled": false
   },
   "source": [
    "compare_optimizers(\n",
    "    F1,\n",
    "    [\n",
    "        ('SGD', SGD, {}),\n",
    "        ('MOMENGTUM SGD', Momentum, {\"gamma\": 0.9}),\n",
    "        ('NAG', NAG, {\"gamma\": 0.8}),\n",
    "        ('RMSProp', RMSProp, {\"gamma\": 0.9}),\n",
    "        ('AdaGrad', AdaGrad, {}),\n",
    "        ('Adadelta', Adadelta, {\"gamma\": 0.001}),\n",
    "        ('Adam', Adam, {\"learning_rate\": 0.035}), \n",
    "    ],\n",
    "    start_point=[3,3],\n",
    "    minima=(0,0), # очевидно, что минимум глобальный и единственный в точке (0, 0)\n",
    "    num_iter=500\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравните эти методы на функции $J(x, y) = x^2sin(x)+y^2sin(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:39:40.984821Z",
     "start_time": "2022-10-06T20:39:40.971450Z"
    }
   },
   "source": [
    "def F2(x, y):\n",
    "    return (x ** 2) * torch.sin(x) + (y ** 2) * torch.sin(y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:40:07.956445Z",
     "start_time": "2022-10-06T20:40:06.602315Z"
    },
    "scrolled": false,
    "tags": []
   },
   "source": [
    "compare_optimizers(\n",
    "    F2,\n",
    "    [\n",
    "        ('SGD', SGD, {}),\n",
    "        ('MOMENTUM SGD', Momentum, {\"gamma\": 0.7}),\n",
    "        ('NAG', NAG, {\"gamma\": 0.8}),\n",
    "        ('RMSProp', RMSProp, {\"gamma\": 0.9}),\n",
    "        ('AdaGrad', AdaGrad, {}),\n",
    "        ('Adadelta', Adadelta, {\"gamma\": 0.001}),\n",
    "        ('Adam', Adam, {\"learning_rate\": 0.01}), \n",
    "    ],\n",
    "    start_point=[2,1],\n",
    "    # очевидно, что глобальных минимумов несколько, оттого точка инициализации будет влиять на схождение\n",
    "    minima=(0,0),\n",
    "    num_iter=500\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравните эти методы на функции $J(x,y)=x^2sin(x^2)+y^2sin(y^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:38.605900Z",
     "start_time": "2022-10-06T18:02:38.602277Z"
    }
   },
   "source": [
    "def F3(x, y):\n",
    "    return (x ** 2) * torch.sin(x ** 2) + (y ** 2) * torch.sin(y ** 2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:40:14.747638Z",
     "start_time": "2022-10-06T20:40:13.286552Z"
    }
   },
   "source": [
    "compare_optimizers(\n",
    "    F3,\n",
    "    [\n",
    "        ('SGD', SGD, {}),\n",
    "        ('MOMENTUM SGD', Momentum, {\"gamma\": 0.9}),\n",
    "        ('NAG', NAG, {\"gamma\": 0.9}),\n",
    "        ('RMSProp', RMSProp, {\"gamma\": 0.9}),\n",
    "        ('AdaGrad', AdaGrad, {}),\n",
    "        ('Adadelta', Adadelta, {\"gamma\": 0.001}),\n",
    "        ('Adam', Adam, {\"learning_rate\": 0.035}), \n",
    "    ],\n",
    "    start_point=[-1,1],\n",
    "    # очевидно, что глобальных минимумов несколько, оттого точка инициализации будет влиять на схождение\n",
    "    minima=(0,0),\n",
    "    num_iter=500\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть 2: Обучение нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравните графики обучения для полносвязной нейросети на методах Adam, Adagrad, AdaDelta, RMSProp, SGD и Mometum SGD (на MNIST).  \n",
    "Для обучения используйте оптимизаторы из первой части, а не из pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:43.509482Z",
     "start_time": "2022-10-06T18:02:40.261018Z"
    }
   },
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T18:02:43.550225Z",
     "start_time": "2022-10-06T18:02:43.521072Z"
    }
   },
   "source": [
    "transform = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('.', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:00:16.210812Z",
     "start_time": "2022-10-06T20:00:16.196535Z"
    }
   },
   "source": [
    "def run_net_learning_process(net, optimizer, train_loader, test_loader, \n",
    "                             epochs, criterion,\n",
    "                             plot=True, verbose=True, conv=False\n",
    "):\n",
    "\n",
    "    train_loss_epochs = []\n",
    "    test_loss_epochs = []\n",
    "    train_accuracy_epochs = []\n",
    "    test_accuracy_epochs = []\n",
    "    try:\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            losses = []\n",
    "            num_of_correct_pred = 0\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                if not conv:\n",
    "                    data = data.view(-1, 28*28) # изменим размер с (batch_size, 1, 28, 28) на (batch_size, 28*28)\n",
    "                optimizer.zero_grad()\n",
    "                net_out = net(data)\n",
    "                loss = criterion(net_out, target)\n",
    "                losses.append(loss.data.item())\n",
    "\n",
    "                pred = net_out.data.max(1)[1]\n",
    "                num_of_correct_pred += pred.eq(target.data).sum().item()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss_epochs.append(np.mean(losses))\n",
    "            train_accuracy_epochs.append(num_of_correct_pred / len(train_loader.dataset))\n",
    "\n",
    "            losses = []\n",
    "            num_of_correct_pred = 0\n",
    "            for data, target in test_loader:\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                if not conv:\n",
    "                    data = data.view(-1, 28 * 28)\n",
    "                net_out = net(data)\n",
    " \n",
    "                loss = criterion(net_out, target)\n",
    "                losses.append(loss.data.item())\n",
    "\n",
    "                # sum up batch loss\n",
    "                pred = net_out.data.max(1)[1]\n",
    "                num_of_correct_pred += pred.eq(target.data).sum().item()\n",
    "\n",
    "            test_loss_epochs.append(np.mean(losses))\n",
    "            test_accuracy_epochs.append(num_of_correct_pred / len(test_loader.dataset))\n",
    "\n",
    "            clear_output(wait=True) # для динамического обновления графиков, wait - очищает вывод\n",
    "\n",
    "            if verbose: # детализация выводимой информации\n",
    "                print(\n",
    "                      f'Network: <{type(net).__name__}>\\n'\n",
    "                      f'Optimizer: <{type(optimizer).__name__}>\\n'\n",
    "                      f'Loss type: <{type(criterion).__name__}>\\n\\n'\n",
    "                      f'Epoch: {epoch+1}\\n'\n",
    "                      f'<Train/Test>\\n'\n",
    "                      f'Loss: {np.round(train_loss_epochs[-1], 3)}/{np.round(test_loss_epochs[-1], 3)} '\n",
    "                      f'| Accuracy: {np.round(train_accuracy_epochs[-1], 3)}/{np.round(test_accuracy_epochs[-1], 3)}'\n",
    "                     )\n",
    "\n",
    "            if plot:\n",
    "                plt.figure(figsize=(12, 5))\n",
    "\n",
    "                # Отображение изменения ошибки\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(train_loss_epochs, label='Train')\n",
    "                plt.plot(test_loss_epochs, label='Test')\n",
    "                plt.xlabel('Epochs', fontsize=16)\n",
    "                plt.ylabel('Loss', fontsize=16)\n",
    "                plt.legend(loc=0, fontsize=16)\n",
    "                plt.grid('on')\n",
    "\n",
    "                # Отображение изменения accuracy\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(train_accuracy_epochs, label='Train')\n",
    "                plt.plot(test_accuracy_epochs, label='Test')\n",
    "                plt.xlabel('Epochs', fontsize=16)\n",
    "                plt.ylabel('Accuracy', fontsize=16)\n",
    "                plt.legend(loc=0, fontsize=16)\n",
    "                plt.grid('on')\n",
    "                plt.show()\n",
    "    except KeyboardInterrupt as KI:\n",
    "        print(KI)\n",
    "\n",
    "    return train_loss_epochs, \\\n",
    "       test_loss_epochs, \\\n",
    "       train_accuracy_epochs, \\\n",
    "       test_accuracy_epochs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:52:55.626658Z",
     "start_time": "2022-10-06T20:52:55.620873Z"
    }
   },
   "source": [
    "def compare_activation_func(loss_results: list, acc_results: list, labels: list) -> None:\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for loss_result, label in zip(loss_results, labels):\n",
    "        plt.plot(loss_result, label=label)\n",
    "\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for acc_result, label in zip(acc_results, labels):\n",
    "        plt.plot(acc_result, label=label)\n",
    "\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:52:55.931326Z",
     "start_time": "2022-10-06T20:52:55.924216Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:52:56.374548Z",
     "start_time": "2022-10-06T20:52:56.360535Z"
    }
   },
   "source": [
    "NUM_EPOCHS=10\n",
    "nets = [FullyConnectedNN() for i in range(7)]\n",
    "print(nets[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:52:56.763687Z",
     "start_time": "2022-10-06T20:52:56.755480Z"
    }
   },
   "source": [
    "optimizers = [\n",
    "                SGD(parameters=nets[0].parameters(), learning_rate=0.01),\n",
    "                Momentum(parameters=nets[1].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                NAG(parameters=nets[2].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                RMSProp(parameters=nets[3].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                Adadelta(parameters=nets[4].parameters(), gamma=0.9),\n",
    "                AdaGrad(parameters=nets[5].parameters(), learning_rate=0.01),\n",
    "                Adam(parameters=nets[6].parameters(), learning_rate=0.005, beta_1=0.9, beta_2=0.99),\n",
    "            ]\n",
    "criterion = nn.NLLLoss()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:55:14.826180Z",
     "start_time": "2022-10-06T20:52:57.093314Z"
    }
   },
   "source": [
    "tr_sgd, ts_sgd, tr_ac_sgd, ts_ac_sgd =\\\n",
    "run_net_learning_process(net=nets[0], optimizer=optimizers[0], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:57:40.781315Z",
     "start_time": "2022-10-06T20:55:14.871379Z"
    }
   },
   "source": [
    "tr_mom, ts_mom, tr_ac_mom, ts_ac_mom =\\\n",
    "run_net_learning_process(net=nets[1], optimizer=optimizers[1], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:00:07.428790Z",
     "start_time": "2022-10-06T20:57:40.816145Z"
    }
   },
   "source": [
    "tr_nag, ts_nag, tr_ac_nag, ts_ac_nag =\\\n",
    "run_net_learning_process(net=nets[2], optimizer=optimizers[2], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:02:47.102925Z",
     "start_time": "2022-10-06T21:00:07.468627Z"
    }
   },
   "source": [
    "tr_rms, ts_rms, tr_ac_rms, ts_ac_rms =\\\n",
    "run_net_learning_process(net=nets[3], optimizer=optimizers[3], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:05:28.209009Z",
     "start_time": "2022-10-06T21:02:47.134579Z"
    }
   },
   "source": [
    "tr_add, ts_add, tr_ac_add, ts_ac_add =\\\n",
    "run_net_learning_process(net=nets[4], optimizer=optimizers[4], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:08:00.061871Z",
     "start_time": "2022-10-06T21:05:28.249615Z"
    }
   },
   "source": [
    "tr_adg, ts_adg, tr_ac_adg, ts_ac_adg =\\\n",
    "run_net_learning_process(net=nets[5], optimizer=optimizers[5], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:10:56.559589Z",
     "start_time": "2022-10-06T21:08:00.090735Z"
    }
   },
   "source": [
    "tr_adm, ts_adm, tr_ac_adm, ts_ac_adm =\\\n",
    "run_net_learning_process(net=nets[6], optimizer=optimizers[6], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:10:56.941901Z",
     "start_time": "2022-10-06T21:10:56.587081Z"
    }
   },
   "source": [
    "compare_activation_func(loss_results=[ts_sgd, ts_mom, ts_nag, ts_rms, ts_add, ts_adg, ts_adm], \n",
    "                        acc_results=[ts_ac_sgd, ts_ac_mom, ts_ac_nag, ts_ac_rms, ts_ac_add, ts_ac_adg, ts_ac_adm],\n",
    "                        labels=[\"SGD\", \"Momentum SGD\", \"NAG\", \"RMSProp\", \"Adadelta\", \"AdaGrad\", \"Adam\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравните графики обучения для сверточной нейросети на методах Adam, Adagrad, AdaDelta, RMSProp, SGD и Mometum SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:10:56.972580Z",
     "start_time": "2022-10-06T21:10:56.967590Z"
    }
   },
   "source": [
    "IMAGE_SIZE = 28\n",
    "CHANNELS = 1\n",
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(ConvClassifier, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(nn.Conv2d(CHANNELS, 3, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n",
    "        self.linear_layers = nn.Sequential(nn.Linear(image_size//2*image_size//2*3, 10), nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:10:56.989973Z",
     "start_time": "2022-10-06T21:10:56.974065Z"
    }
   },
   "source": [
    "NUM_EPOCHS=10\n",
    "cnn_nets = [ConvClassifier(image_size=IMAGE_SIZE) for i in range(7)]\n",
    "print(cnn_nets[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:10:57.038786Z",
     "start_time": "2022-10-06T21:10:57.029295Z"
    }
   },
   "source": [
    "optimizers = [\n",
    "                SGD(parameters=cnn_nets[0].parameters(), learning_rate=0.01),\n",
    "                Momentum(parameters=cnn_nets[1].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                NAG(parameters=cnn_nets[2].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                RMSProp(parameters=cnn_nets[3].parameters(), learning_rate=0.01, gamma=0.9),\n",
    "                Adadelta(parameters=cnn_nets[4].parameters(), gamma=0.9),\n",
    "                AdaGrad(parameters=cnn_nets[5].parameters(), learning_rate=0.01),\n",
    "                Adam(parameters=cnn_nets[6].parameters(), learning_rate=0.005, beta_1=0.9, beta_2=0.99),\n",
    "            ]\n",
    "criterion = nn.NLLLoss()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:13:33.469778Z",
     "start_time": "2022-10-06T21:10:57.097664Z"
    }
   },
   "source": [
    "tr_sgd, ts_sgd, tr_ac_sgd, ts_ac_sgd =\\\n",
    "run_net_learning_process(net=cnn_nets[0], optimizer=optimizers[0], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:16:09.227910Z",
     "start_time": "2022-10-06T21:13:33.501186Z"
    }
   },
   "source": [
    "tr_mom, ts_mom, tr_ac_mom, ts_ac_mom =\\\n",
    "run_net_learning_process(net=cnn_nets[1], optimizer=optimizers[1], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:18:46.915254Z",
     "start_time": "2022-10-06T21:16:09.230149Z"
    }
   },
   "source": [
    "tr_nag, ts_nag, tr_ac_nag, ts_ac_nag =\\\n",
    "run_net_learning_process(net=cnn_nets[2], optimizer=optimizers[2], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:21:26.275137Z",
     "start_time": "2022-10-06T21:18:46.917393Z"
    }
   },
   "source": [
    "tr_rms, ts_rms, tr_ac_rms, ts_ac_rms =\\\n",
    "run_net_learning_process(net=cnn_nets[3], optimizer=optimizers[3], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:24:08.319305Z",
     "start_time": "2022-10-06T21:21:26.276743Z"
    }
   },
   "source": [
    "tr_add, ts_add, tr_ac_add, ts_ac_add =\\\n",
    "run_net_learning_process(net=cnn_nets[4], optimizer=optimizers[4], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:26:47.858722Z",
     "start_time": "2022-10-06T21:24:08.469175Z"
    }
   },
   "source": [
    "tr_adg, ts_adg, tr_ac_adg, ts_ac_adg =\\\n",
    "run_net_learning_process(net=cnn_nets[5], optimizer=optimizers[5], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:29:32.734929Z",
     "start_time": "2022-10-06T21:26:47.894305Z"
    }
   },
   "source": [
    "tr_adm, ts_adm, tr_ac_adm, ts_ac_adm =\\\n",
    "run_net_learning_process(net=cnn_nets[6], optimizer=optimizers[6], epochs=NUM_EPOCHS, criterion=criterion,\n",
    "                         train_loader=train_loader, test_loader=test_loader, \n",
    "                         plot=True, verbose=True, conv=True,\n",
    "                        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:29:33.091718Z",
     "start_time": "2022-10-06T21:29:32.771041Z"
    }
   },
   "source": [
    "compare_activation_func(loss_results=[ts_sgd, ts_mom, ts_nag, ts_rms, ts_add, ts_adg, ts_adm], \n",
    "                        acc_results=[ts_ac_sgd, ts_ac_mom, ts_ac_nag, ts_ac_rms, ts_ac_add, ts_ac_adg, ts_ac_adm],\n",
    "                        labels=[\"SGD\", \"Momentum SGD\", \"NAG\", \"RMSProp\", \"Adadelta\", \"AdaGrad\", \"Adam\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, можно отметить удовлетворительное качество модели с оптимизаторами с найстройками по умолчанию.\n",
    "На полносвязной сети RMSProp явно не смог найти глобальный минимум и сместил свое направление, на сверточной нейронной сети такого эффекта не наблюдается.\n",
    "Заметим, что время, затраченное на обучение как полносвязной, так и сверточной нейросетей не превышает 3 мин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "901.615px",
    "left": "1609.55px",
    "right": "20px",
    "top": "137.979px",
    "width": "416.528px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
