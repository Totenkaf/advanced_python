{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn27NI7zaf9Z"
   },
   "source": [
    "## Семинар 5: \"Улучшение сходимости нейросетей\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY9vse7waf9b"
   },
   "source": [
    "ФИО: Усцов Артем Алексеевич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdEAkdunaf9d"
   },
   "source": [
    "from train_utils import train, compare_results\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIgr4r2Haf9l"
   },
   "source": [
    "На этом семинаре мы попробуем улучшить результаты, полученные на предыдущем занятии\n",
    "Для этого нам понадобятся следующие вещи:\n",
    "* Dropout\n",
    "* Batch Normalization\n",
    "* Инициализация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZHtxmbUaf9m"
   },
   "source": [
    "### Часть 1: Инициализация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8dhNr-faf9n"
   },
   "source": [
    "На лекции доказывалось, что при инициализации He и Glorot дисперсия активаций градиентов в каждом слое будут примерно равны. Давайте проверим это. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "216zZRJAaf9o"
   },
   "source": [
    "# Dataloader\n",
    "to_numpy = lambda x: x.numpy()\n",
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "train_dataset = MNIST('.', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST('.', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqpqV_pVaf9t"
   },
   "source": [
    "images_train, labels_train = next(iter(train_loader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agAN6Tyfaf9x"
   },
   "source": [
    "## Usage example:\n",
    "for X, y in train_loader:\n",
    "    X = X.view(X.size(0), -1)\n",
    "    X = X.numpy() ### Converts torch.Tensor to numpy array\n",
    "    y = y.numpy()\n",
    "    pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "executionInfo": {
     "elapsed": 8767,
     "status": "ok",
     "timestamp": 1602204259023,
     "user": {
      "displayName": "Данила Байгушев",
      "photoUrl": "",
      "userId": "06862651594233453127"
     },
     "user_tz": -180
    },
    "id": "71k9eU4Caf91",
    "outputId": "d056f30d-d7cf-4128-8736-5a83d5511445"
   },
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.Greys_r)\n",
    "    plt.title(y[i])\n",
    "    plt.axis('off')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjxGvxs8af98"
   },
   "source": [
    "<i> 1.1 </i> Инициализируйте полносвязную сеть нормальным шумом N(0, 0.1) с архитектурой 784 -> 500 x (10 раз) -> 10. В качестве активации возьмите tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcWIkH4m-zdC"
   },
   "source": [
    "NUM_EPOCHS = 20"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgCP_qclaf99"
   },
   "source": [
    "def init_layer(layer, mean=0, std=1):\n",
    "    # Тут надо быть аккуратным — можно случайно создать копию и менять значения у копии\n",
    "    weight = layer.state_dict()['weight']\n",
    "    bias = layer.state_dict()['bias']\n",
    "    bias.zero_()\n",
    "    #1 - веса сети\n",
    "    #2 - нули ...\n",
    "    #bias = torch.zeros_like(bias)\n",
    "    weight.normal_(mean=0, std=std)\n",
    "\n",
    "def forward_hook(self, input_, output):\n",
    "    std = input_[0].std().item()\n",
    "    print('forward', std)\n",
    "\n",
    "def backward_hook(self, grad_input, grad_output):\n",
    "    std = grad_input[0].std().item()\n",
    "    print('backward', std)\n",
    "\n",
    "    \n",
    "# пример:\n",
    "layer = nn.Linear(28*28, 10)\n",
    "layer.register_forward_hook(forward_hook)\n",
    "layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# сюда надо подставить другие параметры\n",
    "init_layer(layer, 0.0, 0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 5647,
     "status": "ok",
     "timestamp": 1602204259027,
     "user": {
      "displayName": "Данила Байгушев",
      "photoUrl": "",
      "userId": "06862651594233453127"
     },
     "user_tz": -180
    },
    "id": "roslMRZRaf-C",
    "outputId": "868e8bb3-e2bb-4ebc-da96-31021f57beb1"
   },
   "source": [
    "sizes = [784] + [500] * 10 + [10]\n",
    "layers = []\n",
    "\n",
    "def normal(size_input, size_output):\n",
    "    return 0.1\n",
    "    #return 1\n",
    "\n",
    "def xavier(size_input, size_output):\n",
    "    d = 2 / (size_input + size_output)\n",
    "    return np.sqrt(d)\n",
    "\n",
    "def good_grad(size_input, size_output):\n",
    "    d = 1 / size_output\n",
    "    return np.sqrt(d)\n",
    "\n",
    "#init_func = normal\n",
    "init_func = xavier\n",
    "#init_func = good_grad\n",
    "\n",
    "for size_input, size_output in zip(sizes, sizes[1:]):\n",
    "    \n",
    "    layer = nn.Linear(size_input, size_output)\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "    init_layer(layer, 0.0, init_func(size_input, size_output)) # сюда надо подставить другие параметры\n",
    "    \n",
    "    layers.append(layer)\n",
    "    #layers.append(nn.Tanh())\n",
    "    #layers.append(nn.Sigmoid())\n",
    "    layers.append(nn.Tanh())\n",
    "    \n",
    "print(len(layers))\n",
    "del layers[-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsjtA7N_af-G"
   },
   "source": [
    "<i>1.2 Пропустите батч изображений через нейронную сеть и вычислите дисперсию активаций. Затем вычислите градиент и получите дисперсию градиентов. Сравните эти значения между собой для разных слоев.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3872,
     "status": "ok",
     "timestamp": 1602204259406,
     "user": {
      "displayName": "Данила Байгушев",
      "photoUrl": "",
      "userId": "06862651594233453127"
     },
     "user_tz": -180
    },
    "id": "7Ri1Tvuyaf-K",
    "outputId": "e668298a-0f3e-4dbb-bbe5-84ab7346b44d",
    "scrolled": true
   },
   "source": [
    "network = nn.Sequential(*layers)\n",
    "\n",
    "#пример:\n",
    "n_objects = 100\n",
    "X = images_train[:n_objects].view(n_objects, -1).data\n",
    "y = labels_train[:n_objects].data\n",
    "activations = network(X)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001) \n",
    "loss = loss_fn(activations, y)\n",
    "loss.backward()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUJG1Dbqaf-R"
   },
   "source": [
    "<i>1.3 Повторите эксперимент для инициализаций He и Xavier (формулы есть в лекции).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f0yWUI7-zdE"
   },
   "source": [
    "### Xavier-initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyD1lPKwaf-T"
   },
   "source": [
    "##### YOUR CODE HERE #####\n",
    "def xavier_init_layer(layer, a_range):\n",
    "    weight = layer.state_dict()['weight']\n",
    "    bias = layer.state_dict()['bias']\n",
    "    bias.zero_()\n",
    "    weight.uniform_(-a_range, a_range)\n",
    "    \n",
    "def xavier(size_input, size_output):\n",
    "    d = 6 / (size_input + size_output)\n",
    "    return np.sqrt(d)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mjXHrH4-zdE",
    "outputId": "ca70ff09-1f93-4c38-df6d-574ead321418"
   },
   "source": [
    "sizes = [28*28, 64, 32] + [16] * 30 + [10]\n",
    "layers = []\n",
    "\n",
    "for size_input, size_output in zip(sizes, sizes[1:]):\n",
    "    \n",
    "    layer = nn.Linear(size_input, size_output)\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    xavier_init_layer(layer, xavier(size_input, size_output))\n",
    "    \n",
    "    layers.append(layer)\n",
    "    layers.append(nn.Tanh())\n",
    "    #layers.append(nn.Sigmoid())\n",
    "    #layers.append(nn.ReLU())\n",
    "    \n",
    "print(len(layers))\n",
    "del layers[-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfCUL06E-zdF",
    "outputId": "a07c624e-dd0e-4751-9af9-bcf161b12cd7"
   },
   "source": [
    "network = nn.Sequential(*layers)\n",
    "\n",
    "n_objects = 100\n",
    "X = images_train[:n_objects].view(n_objects, -1).data\n",
    "y = labels_train[:n_objects].data\n",
    "\n",
    "activations = network(X)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "loss = loss_fn(activations, y)\n",
    "loss.backward()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qUOGeNl-zdF"
   },
   "source": [
    "### He-initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mceEy70-zdF"
   },
   "source": [
    "def he_init_layer(layer, mean=0, std=1):\n",
    "    weight = layer.state_dict()['weight']\n",
    "    bias = layer.state_dict()['bias']\n",
    "    bias.zero_()\n",
    "    weight.normal_(mean=mean, std=std)\n",
    "    \n",
    "def he_forward(size_input, size_output):\n",
    "    return np.sqrt(2 / size_input)\n",
    "\n",
    "def he_backward(size_input, size_output):\n",
    "    return np.sqrt(2 / size_output)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OO_Umq5-zdF",
    "outputId": "cbf51684-1687-4237-b352-3697a2dc2baa"
   },
   "source": [
    "sizes = [28*28, 64, 32] + [16] * 30 + [10]\n",
    "layers = []\n",
    "\n",
    "for size_input, size_output in zip(sizes, sizes[1:]):\n",
    "    \n",
    "    layer = nn.Linear(size_input, size_output)\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    #he_init_layer(layer, 0, he_forward(size_input, size_output))\n",
    "    he_init_layer(layer, 0, he_backward(size_input, size_output))\n",
    "    \n",
    "    layers.append(layer)\n",
    "    #layers.append(nn.Tanh())\n",
    "    #layers.append(nn.Sigmoid())\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "print(len(layers))\n",
    "del layers[-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koYmOVfs-zdG",
    "outputId": "06ed3ea1-71e0-4efc-f915-de045845bb50"
   },
   "source": [
    "network = nn.Sequential(*layers)\n",
    "\n",
    "n_objects = 100\n",
    "X = images_train[:n_objects].view(n_objects, -1).data\n",
    "y = labels_train[:n_objects].data\n",
    "\n",
    "activations = network(X)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "loss = loss_fn(activations, y)\n",
    "loss.backward()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U2YSSw5af-Y"
   },
   "source": [
    "<i> 1.4 Сделайте выводы по первой части </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gERtYpzb-zdG"
   },
   "source": [
    "Для функции активации Tanh лучше использовать инициализацию Xavier(во избежание стремительных взрывов и затуханий градиентов), для функции активации ReLU лучше использовать инициализацию весов He.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kYx-hwLaf-b"
   },
   "source": [
    "### Часть 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDoy5_yEaf-c"
   },
   "source": [
    "Другим полезным слоем является __Dropout.__ В нем с вероятностью 1-p зануляется выход каждого нейрона. Этот слой уже реализован в pyTorch, поэтому вновь реализовывать его не интересно.  \n",
    "Давайте реализуем __DropConnect__ — аналог Dropout. В нем с вероятностью 1-p зануляется каждый вес слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAInnQlo-zdG"
   },
   "source": [
    "class TestNetwork(nn.Module):\n",
    "    def __init__(self, final_part):\n",
    "        super().__init__()    \n",
    "        \n",
    "        channels = 1\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(channels, 2, 3, padding=1),    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 4, 3, padding=1),            \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        #input_size = 7 * 7 * 4 = 196\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.final_part = final_part\n",
    "        self.log_softmax = nn.LogSoftmax(1)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.final_part(x)\n",
    "        return self.log_softmax(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxi77m72af-d"
   },
   "source": [
    "<i> 2.1 Реализуйте линейный слой с DropConnect </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee2ljeNCaf-f"
   },
   "source": [
    "# полезная функция: .bernoulli_(p)\n",
    "# не забывайте делать requires_grad=False у маски\n",
    "# помните, что в вычислениях должны участвовать Variable, а не тензоры\n",
    "\n",
    "\n",
    "class DropConnect(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, p=0.5):\n",
    "        super(DropConnect, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):    \n",
    "        mask = torch.zeros_like(self.linear.weight) + self.p\n",
    "        if self.training:          \n",
    "            mask.bernoulli_(self.p)\n",
    "\n",
    "        mask = mask.data\n",
    "        output = F.linear(x, self.linear.weight * mask, self.linear.bias)\n",
    "        return output"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCv4jdyoaf-k"
   },
   "source": [
    "<i> \n",
    "2.2 Сравните графики обучения нейроных сетей:  \n",
    "    \n",
    "1. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и Dropout между всеми слоями;  \n",
    "    \n",
    "2. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и DropConnect вместо всех линейных слоев;  \n",
    "    \n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTYwZi5g-zdH"
   },
   "source": [
    "### Baseline (классическая полносвязная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpAvzFd9-zdH",
    "outputId": "6ca0260f-3dba-448b-f921-49a5b0a44dc9"
   },
   "source": [
    "sizes = [196, 128, 128, 10]\n",
    "layers = []\n",
    "for size_input, size_output in zip(sizes, sizes[1:]):\n",
    "    layers.append(nn.Linear(size_input, size_output))\n",
    "    layers.append(nn.ReLU())\n",
    "\n",
    "# исключим активационную на последнем слое\n",
    "del layers[-1]\n",
    "print()\n",
    "[print(f'{i}: {layer}') for i, layer in enumerate(layers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7B1lu3h-zdI",
    "outputId": "5f7597fb-d495-4ce4-f52f-b65c46212a18"
   },
   "source": [
    "%%time\n",
    "\n",
    "network = TestNetwork(nn.Sequential(*layers))\n",
    "tr, ts, tr_ac, ts_ac = train(network, train_loader, test_loader, NUM_EPOCHS, 0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7bCSwOb-zdI"
   },
   "source": [
    "### Dropout и ReLU после каждого слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55JmZaSs-zdI",
    "outputId": "4768a5f2-f44c-4dce-c653-f43a7dc87cee"
   },
   "source": [
    "sizes = [196, 128, 128, 10]\n",
    "w_dropout_layers = []\n",
    "for size_input, size_output in zip(sizes, sizes[1:]):\n",
    "    w_dropout_layers.append(nn.Linear(size_input, size_output))\n",
    "\n",
    "    # вероятность зануления нейрона в слое=0.3\n",
    "    w_dropout_layers.append(nn.Dropout(0.3))\n",
    "    w_dropout_layers.append(nn.ReLU())\n",
    "\n",
    "# исключим dropout и relu с последних слоев\n",
    "del w_dropout_layers[-2:]\n",
    "print()\n",
    "[print(f'{i}: {layer}') for i, layer in enumerate(w_dropout_layers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 116811,
     "status": "ok",
     "timestamp": 1602204419849,
     "user": {
      "displayName": "Данила Байгушев",
      "photoUrl": "",
      "userId": "06862651594233453127"
     },
     "user_tz": -180
    },
    "id": "QRoKM7rtbGXr",
    "outputId": "5249c849-429c-4c8b-b32a-ef7680da6d4d",
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "\n",
    "network = TestNetwork(nn.Sequential(*w_dropout_layers))\n",
    "tr_1, ts_1, tr_ac_1, ts_ac_1 = train(network, train_loader, test_loader, NUM_EPOCHS, 0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNyaJNK5-zdI"
   },
   "source": [
    "### ReLU и DropConnect вместо линейных слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TboAc_92-zdI",
    "outputId": "13092109-a564-4452-c376-22c0dcba3f91"
   },
   "source": [
    "sizes = [196, 128, 128, 10]\n",
    "dropconnect_layers = [nn.ReLU(), DropConnect(196, 10, p=0.8)]\n",
    "# dropconnect_layers = []\n",
    "# for size_input, size_output in zip(sizes, sizes[1:]):\n",
    "# #     layer = nn.Linear(size_input, size_output)\n",
    "#     dropconnect_layers.append(DropConnect(size_input, size_output, 0.8))\n",
    "#     dropconnect_layers.append(nn.ReLU())\n",
    "\n",
    "# del dropconnect_layers[-1]\n",
    "print()\n",
    "[print(f'{i}: {layer}') for i, layer in enumerate(dropconnect_layers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 112696,
     "status": "ok",
     "timestamp": 1602204552249,
     "user": {
      "displayName": "Данила Байгушев",
      "photoUrl": "",
      "userId": "06862651594233453127"
     },
     "user_tz": -180
    },
    "id": "yTnTgiInaf--",
    "outputId": "73a19917-8e54-474b-a151-1c0426b39a9b",
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "\n",
    "network = TestNetwork(nn.Sequential(*dropconnect_layers))\n",
    "tr_2, ts_2, tr_ac_2, ts_ac_2 = train(network, train_loader, test_loader, NUM_EPOCHS, 0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pooEVZFbaf_L"
   },
   "source": [
    "В test-time стохастичность Dropout убирают и заменяют все веса на их ожидаемое значение: $\\mathbb{E}w = pw + (1-p)0 = pw$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfBC8qS4af_K"
   },
   "source": [
    "<i> 2.3 Сделайте выводы по второй части. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFTfYoSSvm5h"
   },
   "source": [
    "Качество с Dropout лучше, чем с DropConnect.\n",
    "При этом отчетливо видно, что качество модели на трейне много хуже, чем на тесте. Стратегия зануления нейронов или слоев \"затрудняет\" обучение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if-VP-I4af_M"
   },
   "source": [
    "### Часть 3: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vba0rSy-af_N"
   },
   "source": [
    "Наконец, давайте рассмотрим Batch Normalization. Этот слой вычитает среднее и делит на стандартное отклонение. Среднее и дисперсия вычисляются по батчу независимо для каждого нейрона. У этого слоя есть две важные проблемы: его нельзя использовать при обучении с размером батча 1 и он делает элементы батча зависимыми. Давайте реализуем аналог батч нормализации: <a href=https://arxiv.org/pdf/1607.06450.pdf>Layer normalization</a>. В layer normalization среднее и дисперсия вычисляются по активациям нейронов, независимо для каждого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqUTAtjIaf_O"
   },
   "source": [
    "<i> 3.1 Реализуйте Layer Normalization </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2H4VX4_af_O"
   },
   "source": [
    "# полезные функции: .std(dim), .mean(dim)\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(input_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.alpha * (x - x.mean()) / (x.std() + 1e-8) +  self.beta\n",
    "        return output"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2sTZ7h5af_T"
   },
   "source": [
    "<i> \n",
    "3.2 Сравните графики обучения нейроных сетей:  \n",
    "    \n",
    "1. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и Batch normalization между всеми слоями  \n",
    "    \n",
    "2. Свертки из TestNetwork -> 128 -> 128 -> 10 с ReLU и Layer normalization между всеми слоями  \n",
    "    \n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKJn86cI-zdK"
   },
   "source": [
    "### ReLU и Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1YFQVB9-zdK",
    "outputId": "b3402477-050a-4f47-f9e7-2b169416978d"
   },
   "source": [
    "##### YOUR CODE HERE #####\n",
    "sizes = [196, 512, 128, 10]\n",
    "relu_batch_normed_layers = []\n",
    "for in_dim, out_dim in zip(sizes, sizes[1:]): \n",
    "    relu_batch_normed_layers.append(nn.Linear(in_dim, out_dim))\n",
    "    relu_batch_normed_layers.append(nn.BatchNorm1d(out_dim))\n",
    "    relu_batch_normed_layers.append(nn.ReLU())\n",
    "\n",
    "del relu_batch_normed_layers[-2:]\n",
    "print()\n",
    "[print(f'{i}: {layer}') for i, layer in enumerate(relu_batch_normed_layers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1PPqKRZaf_U",
    "scrolled": false,
    "outputId": "0135e822-7266-4a4c-94ba-de5f78f8c632"
   },
   "source": [
    "%%time\n",
    "network = TestNetwork(nn.Sequential(*relu_batch_normed_layers))\n",
    "tr_4, ts_4, tr_ac_4, ts_ac_4 = train(network, train_loader, test_loader, NUM_EPOCHS, 0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvIv1muv-zdK"
   },
   "source": [
    "### ReLU и Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJO0H-3z-zdK",
    "outputId": "c3983091-ed6f-4692-8e66-e4f70c28769f"
   },
   "source": [
    "sizes = [196, 128, 128, 10]\n",
    "relu_layer_normed_layers = []\n",
    "for in_dim, out_dim in zip(sizes, sizes[1:]): \n",
    "    relu_layer_normed_layers.append(nn.Linear(in_dim, out_dim))\n",
    "    relu_layer_normed_layers.append(LayerNormalization(out_dim))\n",
    "    relu_layer_normed_layers.append(nn.ReLU())\n",
    "\n",
    "del relu_layer_normed_layers[-2:]\n",
    "print()\n",
    "[print(f'{i}: {layer}') for i, layer in enumerate(relu_layer_normed_layers)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORLMvse9-zdK",
    "outputId": "a08987ae-18c3-41e1-fb87-93a2d1ff1832"
   },
   "source": [
    "%%time\n",
    "network = TestNetwork(nn.Sequential(*relu_layer_normed_layers))\n",
    "tr_5, ts_5, tr_ac_5, ts_ac_5 = train(network, train_loader, test_loader, NUM_EPOCHS, 0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS6onvnC-zdK"
   },
   "source": [
    "### Часть 4: Сравнение всех методов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUJTwHk_-zdL",
    "outputId": "2d08f0b3-e1a9-4a44-aaf1-a70526617583"
   },
   "source": [
    "compare_results(loss_results=[ts, ts_1, ts_2, ts_4, ts_5], \n",
    "            acc_results=[ts_ac, ts_ac_1, ts_ac_2, ts_ac_4, ts_ac_5],\n",
    "            labels=[\"Baseline\", \"Dropout\", \"Dropconnect\",\n",
    "                    \"BatchNormed\", \"LayerNormed\"],\n",
    "            )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CudyBOIgaf_X"
   },
   "source": [
    "<i> 3.3 Сделайте выводы по третьей части </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgZh16-IvluR"
   },
   "source": [
    "Batch и Layer не несут в себе каких-либо особо видимых различий с точки зрения качества или характера поведения обучения модели, однако Batch оказывается чуть лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4VbIam1-zdL"
   },
   "source": [
    "На графике сравнений, стоит отметить, что методы с нормализацией очень быстро обучаются и выходят на линию \"насыщения\" уже после 1-2 эпохи  \n",
    "А вот, Dropconnect, наоборот ведет себя даже хуже, чем эталанная модель.\n",
    "Возможно, стоит поиграться с вероятностью зануления весов, что так же относится и к подходу Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rtm7aN4B-zdL"
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
