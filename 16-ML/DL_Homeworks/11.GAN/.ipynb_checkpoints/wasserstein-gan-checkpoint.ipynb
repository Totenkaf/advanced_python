{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/cwkx/e85fefe8bffbe3b3598f8f582914eb12/wasserstein-gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0XeNJMELfIb"
   },
   "source": [
    "%%capture\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "!pip install -q torch torchvision livelossplot"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6N22Uz-kLiZW"
   },
   "source": [
    "**Main imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MK1Jl7nkLnPA"
   },
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "run1dh_hM0oO"
   },
   "source": [
    "**Import dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "bK383zeDM4Ac",
    "outputId": "7ee6b198-e7f9-41e2-fd1d-82410a61633a"
   },
   "source": [
    "# helper function to make getting another batch of data easier\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "class_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])),\n",
    "shuffle=True, batch_size=64, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])),\n",
    "shuffle=False, batch_size=64, drop_last=True)\n",
    "\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
    "print(f'> Size of test dataset {len(test_loader.dataset)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-FdW5HnimG2"
   },
   "source": [
    "**View some of the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "BtJs-qxHRLXz",
    "outputId": "342187e3-a52f-4f69-fa9a-860b2056aae1"
   },
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(test_loader.dataset[i][0].permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[test_loader.dataset[i][1]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaTJZNS7kQRI"
   },
   "source": [
    "**Wasserstein Gradient Penalty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X30e0-K3mIiF"
   },
   "source": [
    "def grad_penalty(M, real_data, fake_data, lmbda=10):\n",
    "\n",
    "    alpha = torch.rand(real_data.size(0), 1, 1, 1).to(device)\n",
    "    lerp = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    lerp = lerp.to(device)\n",
    "    lerp.requires_grad = True\n",
    "    lerp_d = M.discriminate(lerp)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=lerp_d, inputs=lerp, grad_outputs=torch.ones(lerp_d.size()).to(device), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    \n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lmbda"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qnjh12UbNFpV"
   },
   "source": [
    "**Define two models: (1) Generator, and (2) Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RGbLY6X-NH4O",
    "outputId": "1f820b4a-d5fd-422d-bc1d-b522a3ed3e3f"
   },
   "source": [
    "# define the model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, f=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, f*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(f*8, f*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(f*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(f*4, f*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(f*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(f*2, f, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(f),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(f, 3, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, f=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminate = nn.Sequential(\n",
    "            nn.Conv2d(3, f, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(f, f*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(f*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(f*2, f*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(f*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(f*4, f*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(f*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(f*8, 1, 4, 2, 1, bias=False)\n",
    "            # nn.Sigmoid() # NOPE! for wasserstein\n",
    "        )\n",
    "        \n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "print(f'> Number of generator parameters {len(torch.nn.utils.parameters_to_vector(G.parameters()))}')\n",
    "print(f'> Number of discriminator parameters {len(torch.nn.utils.parameters_to_vector(D.parameters()))}')\n",
    "\n",
    "# initialise the optimiser\n",
    "optimiser_G = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0.5,0.9))\n",
    "optimiser_D = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0.5,0.9))\n",
    "bce_loss = nn.BCELoss()\n",
    "epoch = 0\n",
    "liveplot = PlotLosses()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1UBl0PJjY-f"
   },
   "source": [
    "**Main training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kb5909Y8D_zx",
    "outputId": "d3d446dd-ceae-4301-d9f1-3c9341ddc1ca"
   },
   "source": [
    "# training loop\n",
    "while (epoch<50):\n",
    "    \n",
    "    # arrays for metrics\n",
    "    logs = {}\n",
    "    gen_loss_arr = np.zeros(0)\n",
    "    dis_loss_arr = np.zeros(0)\n",
    "    grad_pen_arr = np.zeros(0)\n",
    "\n",
    "    # iterate over some of the train dateset\n",
    "    for i in range(200):\n",
    "\n",
    "        # train discriminator k times\n",
    "        for k in range(5):\n",
    "          \n",
    "            x,t = next(train_iterator)\n",
    "            x,t = x.to(device), t.to(device)\n",
    "            optimiser_D.zero_grad()\n",
    "\n",
    "            g = G.generate(torch.randn(x.size(0), 100, 1, 1).to(device))\n",
    "            l_r = D.discriminate(x).mean()\n",
    "            l_r.backward(-1.0*torch.ones(1)[0].to(device)) # real -> -1\n",
    "            l_f = D.discriminate(g.detach()).mean()\n",
    "            l_f.backward(torch.ones(1)[0].to(device)) #  fake -> 1\n",
    "                        \n",
    "            loss_d = (l_f - l_r)\n",
    "            grad_pen = grad_penalty(D, x.data, g.data, lmbda=10)\n",
    "            grad_pen.backward()\n",
    "            \n",
    "            optimiser_D.step()\n",
    "            \n",
    "            dis_loss_arr = np.append(dis_loss_arr, loss_d.item())\n",
    "            grad_pen_arr = np.append(grad_pen_arr, grad_pen.item())\n",
    "        \n",
    "        # train generator\n",
    "        optimiser_G.zero_grad()\n",
    "        g = G.generate(torch.randn(x.size(0), 100, 1, 1).to(device))\n",
    "        loss_g = D.discriminate(g).mean()\n",
    "        loss_g.backward(-1.0*torch.ones(1)[0].to(device)) # fake -> -1\n",
    "        optimiser_G.step()\n",
    "        \n",
    "        gen_loss_arr = np.append(gen_loss_arr, -loss_g.item())\n",
    "\n",
    "    # plot some examples\n",
    "    plt.grid(False)\n",
    "    plt.imshow(torchvision.utils.make_grid(g).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
    "\n",
    "    liveplot.update({\n",
    "        'generator loss': gen_loss_arr.mean(),\n",
    "        'discriminator loss': dis_loss_arr.mean(),\n",
    "        'grad penalty': grad_pen_arr.mean(),\n",
    "    })\n",
    "    liveplot.draw()\n",
    "    sleep(1.)\n",
    "\n",
    "    epoch = epoch+1"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "wasserstein-gan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
