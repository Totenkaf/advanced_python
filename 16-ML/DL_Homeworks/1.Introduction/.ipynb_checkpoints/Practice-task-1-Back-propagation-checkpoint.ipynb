{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 \"Полносвязные нейронные сети\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/Practice-task-1-Back-propagation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО: Усцов Артем Алексеевич, ML-21  \n",
    "Дата: 12 сентября 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать проход \"вперед\" для полносвязной нейронной сети. В дальнейшем мы реализуем процедуру обучения и научим сеть распознавать рукописные цифры.\n",
    "\n",
    "На первой лекции мы познакомились с тем, что такое нейронные сети и изучили три слоя — линейный, сигмоида и SoftMax. Из этих слоев можно составлять глубокие архитектуры и обучать их при помощи градиентного спуска. Чтобы конструировать сложные архитектуры, можно реализовать каждый тип слоя как отдельный \"кирпичик\" и затем собирать полную архитектуру как конструктор. Это мы и попробуем сделать на первом и втором семинарах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый тип слоя мы будем реализовывать при помощи класса, который будет поддерживать три функции: forward, которая будет применять функцию, реализуемую слоем, к входной матрице и backward, которая будет вычислять градиенты и step, которая будет обновлять веса. Чтобы не применять функцию к каждому объекту в отдельности, мы будем подавать на вход слою матрицу размера (N, d), где N — количество объектов, а d — размерность каждого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=backprop.pdf width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция forward будет вычислять по $x$ значение $y$, backward — по $\\frac{\\partial L}{\\partial y}$ вычислять $\\frac{\\partial L}{\\partial x}$ и обновлять внутри себя $\\frac{\\partial L}{\\partial w}$.\n",
    "\n",
    "Важным требованием к реализации является векторизация всех слоев: все операции должны быть сведены к матричным, не должно быть циклов. Это значительно уменьшает временные затраты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:35.095336Z",
     "start_time": "2022-09-13T19:51:34.753533Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Линейный слой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример вычисления градиентов для линейного слоя: $y = Wx$, $x \\in \\mathbb{R}^{K \\times n}$, $y \\in \\mathbb{R}^{K \\times n}$, $W \\in \\mathbb{R}^{n \\times m}$, где $K$ — число объектов.\n",
    "\n",
    "Рассмотрим $L$ как функцию от выходов нейронной сети: $L = L(y_{11}, y_{12}, \\dots)$\n",
    "\n",
    "$$y_{kt} = (Wx)_{kt} = \\sum_{z=1}^{n} x_{kz}W_{zt}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial y_{kt}}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial \\sum_z x_{kz}w_{zt}}{\\partial x_{ij}}= \\sum_{t} \\frac{\\partial L}{\\partial y_{it}}\\frac{\\partial w_{jt}}{\\partial x_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial x} = \\frac{\\partial{L}}{\\partial y}W^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:36.742294Z",
     "start_time": "2022-09-13T19:51:36.736413Z"
    }
   },
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Creates weights and biases for linear layer.\n",
    "        Dimention of inputs is *input_size*, of output: *output_size*.\n",
    "        '''   \n",
    "        #### YOUR CODE HERE\n",
    "        #### Create weights, initialize them with samples from N(0, 0.1).\n",
    "        self.W = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.b = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, input_size).\n",
    "        Returns output of size (N, output_size).\n",
    "        Hint: You may need to store X for backward pass\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        self.X = X\n",
    "        self.Y = X @ self.W + self.b\n",
    "        return self.Y\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdw and dLdx.\n",
    "        2. Store dLdw for step() call\n",
    "        3. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        self.dLdW = self.X.T.dot(dLdy)\n",
    "        self.dLdb = dLdy.sum(0)\n",
    "        dLdx = dLdy @ self.W.T\n",
    "        \n",
    "        return dLdx\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        '''\n",
    "        1. Apply gradient dLdw to network:\n",
    "        w <- w - l*dLdw\n",
    "        '''\n",
    "        self.W -= self.dLdW * learning_rate\n",
    "        self.b -= self.dLdb * learning_rate"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2: Численный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Релизуйте функцию проверки численного градиента. Для этого для каждой переменной, по которой считается градиент, надо вычислить численный градиент: $f'(x) \\approx \\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$. Функция должна возвращать максимальное абсолютное отклонение аналитического градиента от численного. В качестве $\\epsilon$ рекомендуется взять $10^{-6}$. При правильной реализации максимальное отличие будет иметь порядок $10^{-8}-10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:37.819603Z",
     "start_time": "2022-09-13T19:51:37.811052Z"
    }
   },
   "source": [
    "EPS = 1e-6\n",
    "def check_gradient(func, X, gradient):\n",
    "    '''\n",
    "    Computes numerical gradient and compares it with analytcal.\n",
    "    func: callable, function of which gradient we are interested. Example call: func(X)\n",
    "    X: np.array of size (n x m)\n",
    "    gradient: np.array of size (n x m)\n",
    "    Returns: maximum absolute diviation between numerical gradient and analytical.\n",
    "    '''\n",
    "    #### YOUR CODE HERE\n",
    "    grad_X = np.zeros(X.shape)\n",
    "    for i in range(grad_X.shape[0]):\n",
    "        for j in range(grad_X.shape[1]):\n",
    "            X[i, j] += EPS\n",
    "            f_plus = func(X)\n",
    "            X[i, j] -= EPS\n",
    "            \n",
    "            X[i, j] -= EPS\n",
    "            f_minus = func(X)\n",
    "            X[i, j] += EPS\n",
    "            \n",
    "            grad_X[i, j] = (f_plus - f_minus) / (2 * EPS)\n",
    "    \n",
    "    return np.max(np.abs(gradient - grad_X))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:38.366360Z",
     "start_time": "2022-09-13T19:51:38.356492Z"
    }
   },
   "source": [
    "# Toy-test\n",
    "np.random.seed(777)\n",
    "x = np.random.rand(10, 20)\n",
    "func = lambda x: (x**2).sum()\n",
    "\n",
    "gradient = 2*x\n",
    "check_gradient(func, x, gradient)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте линейный слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$ и $\\frac{\\partial L}{\\partial w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:40.625372Z",
     "start_time": "2022-09-13T19:51:40.616201Z"
    }
   },
   "source": [
    "np.random.seed(777)\n",
    "X = np.array([\n",
    "              [0, 1], \n",
    "              [1, 1]\n",
    "             ])\n",
    "\n",
    "Y = np.array([[0], \n",
    "              [1]])\n",
    "l = Linear(2, 1)\n",
    "\n",
    "def loss(W):\n",
    "    l.W = W\n",
    "    prediction = l.forward(X)\n",
    "    return np.sum((prediction - Y)**2)\n",
    "\n",
    "prediction = l.forward(X)\n",
    "print(f\"Predictions:\\n {prediction}\\n\")\n",
    "\n",
    "dLdy = 2*(prediction - Y)\n",
    "print(f\"Derivative weights dLdy:\\n {dLdy}\\n\")\n",
    "\n",
    "l.backward(dLdy)\n",
    "print(f\"Backward weights W:\\n {l.dLdW}\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:41.717306Z",
     "start_time": "2022-09-13T19:51:41.713592Z"
    }
   },
   "source": [
    "pass_score = check_gradient(loss, l.W, l.dLdW)\n",
    "if pass_score < EPS:\n",
    "    print(f\"CHECK PASSED.\\nSCORE: {pass_score}\")\n",
    "else: print(f\"CHECK FAULT.\\nSCORE: {pass_score}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3: Сигмоида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:43.741620Z",
     "start_time": "2022-09-13T19:51:43.739155Z"
    }
   },
   "source": [
    "def sigmoid(X):\n",
    "    return 1.0 / (1 + np.exp(-X))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:44.361887Z",
     "start_time": "2022-09-13T19:51:44.358495Z"
    }
   },
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, d)\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        self.sigm_out = sigmoid(X)\n",
    "        return self.sigm_out\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdx.\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        dLdX = dLdy * self.sigm_out * (1 - self.sigm_out)\n",
    "        return dLdX\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:45.733537Z",
     "start_time": "2022-09-13T19:51:45.730162Z"
    }
   },
   "source": [
    "X = np.random.rand(2, 2)\n",
    "sigm = Sigmoid()\n",
    "\n",
    "def sigm_loss(X):\n",
    "    l = sigm.forward(X)\n",
    "    return np.sum(l ** 2)\n",
    "\n",
    "l = sigm.forward(X)\n",
    "dLdy = 2 * l\n",
    "gradients = sigm.backward(dLdy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T19:51:46.636369Z",
     "start_time": "2022-09-13T19:51:46.632560Z"
    }
   },
   "source": [
    "sigm_pass_score = check_gradient(sigm_loss, X, gradients)\n",
    "if sigm_pass_score < EPS:\n",
    "    print(f\"CHECK PASSED.\\nSCORE: {sigm_pass_score}\")\n",
    "else: print(f\"CHECK FAULT.\\nSCORE: {sigm_pass_score}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4: Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы увидели на семинаре, вычисление производной для связки SoftMax + Negative log-likelihood проще чем для этих двух слоев по отдельности. Поэтому мы реализуем их как один класс. Важное замечание: на проходе \"вперед\" важно воспользоваться трюком <a href=\"https://blog.feedly.com/tricks-of-the-trade-logsumexp/\">log-sum-exp</a>, чтобы не столкнуться с численными неустойчивостями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:03:32.871599Z",
     "start_time": "2022-09-13T20:03:32.855629Z"
    }
   },
   "source": [
    "class NLLLoss:\n",
    "    def __init__(self, mode=\"softmax\"):\n",
    "        '''\n",
    "        Applies Softmax operation to inputs and computes NLL loss\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### (Hint: No code is expected here, just joking)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, C), where C is the number of classes\n",
    "        y is np.array of size (N), contains correct labels\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "\n",
    "        self.n_samples = X.shape[0] \n",
    "        self.n_classes = X.shape[1]\n",
    "\n",
    "        self.Z = self.softmax(X)\n",
    "        self.Y = self.one_hot(y)\n",
    "        L = self.cross_entropy(self.Z, self.Y)\n",
    "\n",
    "        return L\n",
    "    \n",
    "    def log_sum_exp(self, X):\n",
    "        # Максимальный элемент внутри каждой строчки, оттого axis=1, keepdims=True\n",
    "        c = np.max(X, axis=1, keepdims=True)\n",
    "        return c + np.log(np.sum(np.exp(X - c), axis=1, keepdims=True))\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Tranforms matrix of predicted scores to matrix of probabilities\n",
    "        Args:\n",
    "            scores: numpy array of shape (n_samples, n_classes)\n",
    "            with unnormalized scores\n",
    "        Returns:\n",
    "            softmax: numpy array of shape (n_samples, n_classes)\n",
    "            with probabilities\n",
    "        \"\"\"\n",
    "        if self.mode == \"lse\":\n",
    "            result = np.exp(X - self.log_sum_exp(X))\n",
    "        else:\n",
    "            exp = np.exp(X)\n",
    "            # Суммируем внутри каждой строчки, оттого axis=1, keepdims=True\n",
    "            sum_exp = np.sum(exp, axis=1, keepdims=True)\n",
    "            result = exp / sum_exp\n",
    "            # Матрица вероятностей - вероятность принадлежности конкретного объкета (строки), \n",
    "            # к конкретному классу (столбцу)\n",
    "        return result\n",
    "\n",
    "    def cross_entropy(self, X, y):\n",
    "        # При домножении маски меток на матрицу вероятностей, получаем матрицу,\n",
    "        # в которой на одной позиции лосс, на остальных - нули\n",
    "        # Сложим через sum(1) внутри строки, получим вектор лоссов на каждом объекте\n",
    "        # Усредняем по вектору через mean(0), лосс - скаляр\n",
    "        loss = -(np.log(self.Z) * self.Y).sum(1).mean(0)\n",
    "        return loss\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        \"\"\"\n",
    "        Tranforms vector y of labels to one-hot encoded matrix\n",
    "        \"\"\"\n",
    "        # Создание маски для взодящего датасета, пометка 1, где присутствует метка\n",
    "        one_hot = np.zeros((self.n_samples, self.n_classes))\n",
    "        one_hot[np.arange(self.n_samples), y.T] = 1\n",
    "        return one_hot \n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Note that here dLdy = 1 since L = y\n",
    "        1. Compute dLdx\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        # Стандартная производная от -log(softmax(x))\n",
    "        # Бьем из скаляра в матрицы, оттого производная - матрица\n",
    "        dLdx = (self.Z - self.Y) / self.Y.shape[0]\n",
    "        return dLdx"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:03:56.643077Z",
     "start_time": "2022-09-13T20:03:56.629856Z"
    }
   },
   "source": [
    "X = np.random.rand(10, 2)\n",
    "y = np.random.randint(0, 2, 10)\n",
    "print(f\"X:\\n {X}\")\n",
    "print(f\"Y:\\n {y}\")\n",
    "\n",
    "L = NLLLoss(mode=\"lse\")\n",
    "def f(X):\n",
    "    l = L.forward(X,y)\n",
    "    return l\n",
    "\n",
    "l = L.forward(X, y)\n",
    "gradients = L.backward()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:03:57.506447Z",
     "start_time": "2022-09-13T20:03:57.496437Z"
    }
   },
   "source": [
    "l"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:03:58.585640Z",
     "start_time": "2022-09-13T20:03:58.575002Z"
    }
   },
   "source": [
    "gradients"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:03:59.153619Z",
     "start_time": "2022-09-13T20:03:59.141497Z"
    }
   },
   "source": [
    "pass_score = check_gradient(f, X, gradients)\n",
    "if pass_score < EPS:\n",
    "    print(f\"CHECK PASSED.\\nSCORE: {pass_score}\")\n",
    "else: print(f\"CHECK FAULT.\\nSCORE: {pass_score}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5, нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть \"кирпичики\", мы можем написать класс, который будет собирать всю сеть вместе "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:07:49.826888Z",
     "start_time": "2022-09-13T20:07:49.816545Z"
    }
   },
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, modules):\n",
    "        '''\n",
    "        Constructs network with *modules* as its layers\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        self.layers = list(modules)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layers to input\n",
    "        for layer in self.layers:\n",
    "            # Прокидываем на следующий слой\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        dLdy here is a gradient from loss function\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        # Должны идти с последнего до первого слоя\n",
    "        for layer in self.layers[::-1]:\n",
    "            # Прокидываем на предыдущий слой\n",
    "            dLdy = layer.backward(dLdy)\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            # Прокидываем на следующий слой\n",
    "            layer.step(learning_rate)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 6, обучение на простых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:07:52.069025Z",
     "start_time": "2022-09-13T20:07:51.857144Z"
    }
   },
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data['arr_0'], data['arr_1']\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:07:55.999307Z",
     "start_time": "2022-09-13T20:07:55.988637Z"
    }
   },
   "source": [
    "nn = NeuralNetwork(\n",
    "                    [\n",
    "                        Linear(2, 10), \n",
    "                        Sigmoid(), \n",
    "                        Linear(10, 10), \n",
    "                        Sigmoid(), \n",
    "                        Linear(10, 3)\n",
    "                    ])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:08:11.876170Z",
     "start_time": "2022-09-13T20:08:01.823007Z"
    }
   },
   "source": [
    "NUM_EPOCHS = 30000\n",
    "# Используем численную оптимизацию\n",
    "loss_f = NLLLoss(mode=\"lse\")\n",
    "\n",
    "losses = []\n",
    "# Производим обучение на всей выборке, батчи не используем\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    predictions = nn.forward(X)\n",
    "    loss = loss_f.forward(predictions, y)\n",
    "    losses.append(loss)\n",
    "    dLdy = loss_f.backward()\n",
    "    nn.backward(dLdy)\n",
    "    # Делаем шаг в сторону антиградиента. Варьируем\n",
    "    nn.step(0.05)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss diminishing by Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f\"Minimal loss = {losses[-1]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:08:13.716317Z",
     "start_time": "2022-09-13T20:08:13.705421Z"
    }
   },
   "source": [
    "predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:08:14.185275Z",
     "start_time": "2022-09-13T20:08:14.179590Z"
    }
   },
   "source": [
    "y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:08:15.945133Z",
     "start_time": "2022-09-13T20:08:15.926160Z"
    }
   },
   "source": [
    "np.argmax(predictions, axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:08:17.512329Z",
     "start_time": "2022-09-13T20:08:17.274371Z"
    }
   },
   "source": [
    "N_mesh = 50\n",
    "x_grid = np.linspace(0.0, 1.2, N_mesh)\n",
    "y_grid = np.linspace(0, 1, N_mesh)\n",
    "\n",
    "# Разобьем пространство объектов на батчи\n",
    "X_mesh, Y_mesh = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "mesh = np.dstack((X_mesh, Y_mesh)).reshape(N_mesh * N_mesh, 2)\n",
    "# Нейросеть с neglogsoftmax() возвращает вероятности принадлежности классу. Преобразуем в метки\n",
    "predictions = np.argmax(nn.forward(mesh), 1)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Predictions Interpretation with NLLLoss\")\n",
    "plt.scatter(mesh[:, 0], mesh[:, 1], c=predictions, alpha=0.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите архитектуру вида 2 -> 10 -> 10 -> 3:\n",
    "* Linear(2, 10)\n",
    "* Sigmoid()\n",
    "* Linear(10, 10)\n",
    "* Sigmoid()\n",
    "* Linear(10, 3)\n",
    "\n",
    "В качестве функции потерь используйте NLLLoss.\n",
    "1. Создайте сеть, в цикле запускайте forward, backward, step (используйте learning rate 0.005). \n",
    "2. Нарисуйте график сходимости (величина NLL после каждого обновления).\n",
    "3. Нарисуйте разделяющую поверхность\n",
    "4. Попробуйте подобрать темп обучения. Как меняется сходимость?  \n",
    "__При выборе темпа обучения в 0,05, на 30000 эпохе потери сводятся практически к нулю__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличная визуализация: http://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Подберите темп обучения. Как меняется сходимость? Нарисуйте график оптимального значения функции потерь для различных значений learning_rate\n",
    "* Решите поставленную выше задачу как задачу регрессии с MSE. Изменилась ли разделяющая поверхность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:09:32.749230Z",
     "start_time": "2022-09-13T20:09:32.744791Z"
    }
   },
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, C), where C is the number of classes\n",
    "        y is np.array of size (N), contains correct labels\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.y = y.reshape(y.shape[0],-1)\n",
    "        return np.sum((self.X - self.y) ** 2) / self.X.shape[0]\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Note that here dLdy = 1 since L = y\n",
    "        1. Compute dLdx\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        dLdx = 2 * (self.X - self.y) / self.y.shape[0]\n",
    "        return dLdx"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:11:51.170160Z",
     "start_time": "2022-09-13T20:09:33.867251Z"
    }
   },
   "source": [
    "rates = np.linspace(0.005, 0.05, 20)\n",
    "losses = []\n",
    "network = NeuralNetwork(\n",
    "                        [Linear(2, 10), \n",
    "                         Sigmoid(), \n",
    "                         Linear(10, 10), \n",
    "                         Sigmoid(), \n",
    "                         Linear(10, 3)\n",
    "                        ])\n",
    "\n",
    "for rate in rates:\n",
    "    loss_f = MSELoss()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        predictions = network.forward(X)\n",
    "        loss = loss_f.forward(predictions, y)\n",
    "        dLdy = loss_f.backward()\n",
    "        network.backward(dLdy)\n",
    "        network.step(rate)\n",
    "    losses.append(loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:11:51.536385Z",
     "start_time": "2022-09-13T20:11:51.385758Z"
    }
   },
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(rates, losses)\n",
    "plt.title(\"Losses by rates\")\n",
    "plt.xlabel(\"Rate\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f\"Minimal loss = {losses[-1]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:11:51.730346Z",
     "start_time": "2022-09-13T20:11:51.724320Z"
    }
   },
   "source": [
    "network.forward(mesh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:11:51.937411Z",
     "start_time": "2022-09-13T20:11:51.931866Z"
    }
   },
   "source": [
    "np.argmax(network.forward(mesh), 1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:11:52.205469Z",
     "start_time": "2022-09-13T20:11:52.199165Z"
    }
   },
   "source": [
    "(np.rint(network.forward(mesh)))[:, 0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T20:11:52.636780Z",
     "start_time": "2022-09-13T20:11:52.408676Z"
    }
   },
   "source": [
    "N_mesh = 50\n",
    "x_grid = np.linspace(0, 1.2, N_mesh)\n",
    "y_grid = np.linspace(0, 1, N_mesh)\n",
    "\n",
    "X_mesh, Y_mesh = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "mesh = np.dstack((X_mesh, Y_mesh)).reshape(N_mesh * N_mesh, 2)\n",
    "# Так как работаем уже не с вероятностями, а простой метрикой, будем округлять до ближайшего целого числа\n",
    "predictions = (np.rint(network.forward(mesh)))[:, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Predictions Interpretation with MSELoss\")\n",
    "plt.scatter(mesh[:, 0], mesh[:, 1], c=predictions, alpha=0.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
