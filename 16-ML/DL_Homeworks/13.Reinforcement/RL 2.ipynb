{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "colab": {
   "name": "Seminar_15.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5d4e299a35694e3a9300d0c84538f1a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_8089f6876ed248cab140863c56f9651b",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_a2f82c333cd7401786792907fdf7f1a2",
       "IPY_MODEL_b66a71b128194ab19f10edcef461b4a1"
      ]
     }
    },
    "8089f6876ed248cab140863c56f9651b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "a2f82c333cd7401786792907fdf7f1a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_f1361eb37afe40ed931f0896e4d46170",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_75645aab92b54006a59754051d15ba5e"
     }
    },
    "b66a71b128194ab19f10edcef461b4a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_90751e0807cb4709a81747ccc973d59e",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 2000/? [06:43&lt;00:00,  4.96it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_21f623d9da054330b247a195415ddde0"
     }
    },
    "f1361eb37afe40ed931f0896e4d46170": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "75645aab92b54006a59754051d15ba5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "90751e0807cb4709a81747ccc973d59e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "21f623d9da054330b247a195415ddde0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcclPSAKuK_u"
   },
   "source": [
    "## Семинар 15: \"Обучение с подкреплением 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYllaCDeuK_v"
   },
   "source": [
    "ФИО: Калашников Дмитрий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6p08QrRuK_w"
   },
   "source": [
    "###  FrozenLake\n",
    "\n",
    "\n",
    "<img src=\"http://vignette2.wikia.nocookie.net/riseoftheguardians/images/4/4c/Jack's_little_sister_on_the_ice.jpg/revision/latest?cb=20141218030206\" alt=\"a random image to attract attention\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wBtCUYyHuK_x"
   },
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnPgPH2puK_y",
    "outputId": "c4ac9a20-6baa-4928-fa47-5101252fe49d"
   },
   "source": [
    "# display the game state\n",
    "env.render()"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCyTMya1uK_z"
   },
   "source": [
    "### legend\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/800/1*MCjDzR-wfMMkS0rPqXSmKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS1P5DvxuK_0"
   },
   "source": [
    "## Задание 1.\n",
    "Подберите значения alpha и epsilon и найдите приближение оптимальной Q-функции для Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C42fHVzJuK_0"
   },
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        oldv = self.q.get((state, action), None)\n",
    "\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bXtzfJA9uK_2"
   },
   "source": [
    "def run_episode_qlearn_learn(env, qlearn, gamma = 1.0, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = qlearn.chooseAction(obs)\n",
    "        obs_new, reward, done, _ = env.step(action)\n",
    "        qlearn.learn(obs, action, reward, obs_new)\n",
    "        obs = obs_new\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hxMCZ5GmuK_3"
   },
   "source": [
    "def run_episode_qlearn(env, qlearn, gamma = 1.0, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = qlearn.chooseAction(obs)\n",
    "        obs_new, reward, done, _ = env.step(action)\n",
    "        obs = obs_new\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "y3HxtwocuK_5"
   },
   "source": [
    "def evaluate_qlearn(env, qlearn, gamma = 1.0, n = 100):\n",
    "    scores = [\n",
    "            run_episode_qlearn(env, qlearn, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "62lBH1K_zOln"
   },
   "source": [
    "def qlearning(env, qlearn, gamma = 1.0, render = False, n_epochs=10, verbose=False):\n",
    "    for epoch in range(n_epochs):\n",
    "        total_reward = run_episode_qlearn(env, qlearn, gamma, render)\n",
    "        if verbose and epoch % 100 == 0:\n",
    "            print(f'reward {total_reward} for {epoch} epochs')"
   ],
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rd_YFnMG1T8O"
   },
   "source": [
    "gamma = 1.0\n",
    "n_epochs = 1000\n",
    "epsilon = 0.21 # 0.1\n",
    "alpha = 0.89 # 0.2"
   ],
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ie6aTbAn3xlI"
   },
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm"
   ],
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "5d4e299a35694e3a9300d0c84538f1a7",
      "8089f6876ed248cab140863c56f9651b",
      "a2f82c333cd7401786792907fdf7f1a2",
      "b66a71b128194ab19f10edcef461b4a1",
      "f1361eb37afe40ed931f0896e4d46170",
      "75645aab92b54006a59754051d15ba5e",
      "90751e0807cb4709a81747ccc973d59e",
      "21f623d9da054330b247a195415ddde0"
     ]
    },
    "id": "XO8JJLR-2SSF",
    "outputId": "a241c115-542b-4972-816c-7d68d520912d"
   },
   "source": [
    "results = []\n",
    "for gamma, epsilon, alpha in tqdm(itertools.product( np.linspace(0, 1.0, 5), np.linspace(0, 1.0, 20), np.linspace(0, 1.0, 20) )):\n",
    "    qlearn = QLearn(actions=range(env.env.nA), gamma=gamma, epsilon=epsilon, alpha=alpha)\n",
    "    env = gym.make(\"FrozenLake-v0\")\n",
    "    qlearning(env.env, qlearn, gamma, render=False, n_epochs=n_epochs)\n",
    "    q_score = evaluate_qlearn(env, qlearn, gamma = gamma, n = 100)\n",
    "    results.append([q_score, gamma, epsilon, alpha])"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YoXlmvRk4LJN",
    "outputId": "a8991c6f-0093-4e1f-fd95-379afe8d3255"
   },
   "source": [
    "sorted(results, reverse=True)[:15]"
   ],
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "thiwhhol6Ojx"
   },
   "source": [
    "gamma = 1.0\n",
    "n_epochs = 1000\n",
    "epsilon = 0.21\n",
    "alpha = 0.89"
   ],
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYyfzTl16XaB",
    "outputId": "c0ece817-db85-4308-8f26-1e14795793d8"
   },
   "source": [
    "qlearn = QLearn(actions=range(env.env.nA), gamma=gamma, epsilon=epsilon, alpha=alpha)\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "qlearning(env.env, qlearn, gamma, render=False, n_epochs=n_epochs)\n",
    "q_score = evaluate_qlearn(env, qlearn, gamma = gamma, n = 100)\n",
    "q_score"
   ],
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VEJu0iiVuK_7"
   },
   "source": [
    "## Задание 2.\n",
    "Обучите сеть DQN для среды http://gym.openai.com/envs/Pong-v0/ "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJgiyj-D81lZ",
    "outputId": "4e0e0813-c5ad-4f68-aac7-ae13f9d9ed44"
   },
   "source": [
    "pip install atari-py "
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yfBZnlCx8_OB"
   },
   "source": [
    "import requests\n",
    "r = requests.get('http://www.atarimania.com/roms/Roms.rar')\n",
    "with open('Roms.rar', 'wb') as fout:\n",
    "    fout.write(r.content)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i-4iKgFk9WzO"
   },
   "source": [
    "!unrar x Roms.rar\n",
    "!mkdir Roms\n",
    "!mv ROMS.zip \"HC ROMS.zip\" Roms\n",
    "!python -m atari_py.import_roms Roms"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRu1NOTRYf2Z"
   },
   "source": [
    "### Обработка фреймов на уровне observation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wSUUC5iHaHRn"
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1P2nuQJ660Qg"
   },
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\" Wrapper for starting the game firstly \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "99r6_spilDR7"
   },
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7fDWho7AlFo6"
   },
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "\n",
    "        return x_t.astype(np.uint8)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JvhJMldCmgz-"
   },
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OvFTGTPzmjI-"
   },
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8nYFzAhJmlDe"
   },
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGuKWjnwmlVe"
   },
   "source": [
    "def create_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDuaoELUnMPu"
   },
   "source": [
    "### Сеть"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3pkF4ydCZZKw"
   },
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=3),\n",
    "            nn.LayerNorm( (82, 82) ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.LayerNorm( (80, 80) ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.LayerNorm( (38, 38) ),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(128, 256, kernel_size=3),\n",
    "            # nn.LayerNorm( (36, 36) ),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Conv2d(256, 512, kernel_size=3),\n",
    "            # nn.LayerNorm( (16, 16) ),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        conv_out_size = 38 * 38 * 128 #  16 * 16 * 512 \n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 1024), # 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ],
   "execution_count": 118,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDRuj4X5nOef"
   },
   "source": [
    "### Классы для объектов из алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iOBbq-GEZhQA"
   },
   "source": [
    "MEAN_REWARD_BOUND = 15.0\n",
    "\n",
    "gamma = 0.95                \n",
    "batch_size = 64          \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4      \n",
    "sync_target_frames = 10000\n",
    "replay_start_size = 5000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.9999\n",
    "eps_min=0.03"
   ],
   "execution_count": 120,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9clnjFwvZjmD"
   },
   "source": [
    "class ExperienceReplay:\n",
    "    \"\"\" Experience Replay buffer for experience sampling from algorithm \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ],
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "olZX8ldaZiGp"
   },
   "source": [
    "class Agent:\n",
    "    \"\"\" Agent from algorithm \"\"\"\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = self.env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = (self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ],
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g969idRxFX9n"
   },
   "source": [
    "### Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J4GAiOmfZoOr"
   },
   "source": [
    "def run():\n",
    "\n",
    "    DEFAULT_ENV_NAME = \"Pong-v0\"\n",
    "    env = create_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    buffer = ExperienceReplay(replay_size)\n",
    "    agent = Agent(env, buffer)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    epsilon = eps_start\n",
    "    total_rewards = []\n",
    "    frame_idx = 0  \n",
    "\n",
    "    best_mean_reward = None\n",
    "    MAX_GAMES = 30\n",
    "\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(epsilon * eps_decay, eps_min)\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "\n",
    "        if reward is not None:\n",
    "\n",
    "            total_rewards.append(reward)\n",
    "            mean_reward = np.mean(total_rewards[-20:])\n",
    "\n",
    "            if len(total_rewards) % 1 == 0:\n",
    "                print(f\"game {len(total_rewards)} - reward {mean_reward}\")\n",
    "\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-final.pth\")\n",
    "                best_mean_reward = mean_reward\n",
    "                if best_mean_reward is not None:\n",
    "                    print(f\"Best mean reward updated {best_mean_reward}\")\n",
    "\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                break\n",
    "\n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "\n",
    "        batch = buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        states_v = torch.tensor(states).to(device)\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        next_state_values = target_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "        \n",
    "        if len(total_rewards) >= MAX_GAMES:\n",
    "            break\n",
    "    \n",
    "    if len(total_rewards) > MAX_GAMES:\n",
    "        print('Hasnt converged')\n",
    "    \n",
    "    return env, target_net"
   ],
   "execution_count": 123,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srgqhAkla-fX",
    "outputId": "51c5c934-1549-483a-92ec-51a0421d04a2"
   },
   "source": [
    "env, target_net = run()"
   ],
   "execution_count": 124,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw8eBtOUFH6x"
   },
   "source": [
    "### Тестируем"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q_o4FwM1zIIG"
   },
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 125,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "6FeEEaeyc2ax",
    "outputId": "99f03b13-8525-46e5-8aae-89b5f2c1e713"
   },
   "source": [
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "#net = target_net \n",
    "net.load_state_dict(torch.load('Pong-v0-final.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "while True:\n",
    "\n",
    "    state_v = torch.tensor(np.array([state], copy=False))\n",
    "    q_vals = net(state_v.to('cuda')).data.cpu().numpy()[0]\n",
    "    action = np.argmax(q_vals)\n",
    "\n",
    "    clear_output(True)\n",
    "\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    plt.figure(figsize=(5,8))\n",
    "    plt.imshow(screen.cpu().squeeze(0).numpy(), interpolation='none')\n",
    "    plt.show()\n",
    "\n",
    "    #time.sleep(0.01)\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total reward: %.2f\" % total_reward)"
   ],
   "execution_count": 128,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vmYZcO8QA1J-"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
