{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR5K955oK7Mq"
   },
   "source": [
    "## Семинар 3 \"Определение ключевых точек лица\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPNbPLlPK7M6"
   },
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCmzCGjSK7M7"
   },
   "source": [
    "Используя датасет из архива решите задачу регрессии для поиска 68-ми ключевых точек лица. \n",
    "\n",
    "Для обучения и валидации используйте изображения и разметку из папок train и test соответственно.\n",
    "\n",
    "Для **зачета** дз нужно сделать одно из двух:\n",
    "\n",
    "1. Исследовать 6 разных архитектур и написать подробный вывод + мысли почему одна сеть работает лучше чем другая (разные функции активации, глубины сетей, размеры и типы слоев)\n",
    "2. Преодолеть порог MSE=8 на тесте\n",
    "\n",
    "\n",
    "Также до  22 марта 23:59 можно получить до 5 дополнительных баллов за лучшее решение. (За первое место 5 баллов, за второе — 3 балла, за третье — 1 балла.)\n",
    "\n",
    "Для решения можно пользоваться всем инструментарием, с которым мы познакомимся:  предобученные модели, сверточные сети, любые оптимизаторы, batchnorm, dropout, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc-EHqs9LCAy",
    "outputId": "373a57f0-039f-420e-9f73-2ca05043022f"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-n_Yh8gPK7M8"
   },
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn import MSELoss, Sequential, Linear, Sigmoid, Tanh, L1Loss, Module, Parameter\n",
    "from torch.nn import Flatten,Dropout, LayerNorm, Sequential,\\\n",
    "                        Module, Conv2d, BatchNorm2d, ReLU, MaxPool2d, BatchNorm1d\n",
    "from torch.autograd import Variable\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "from skimage import filters\n",
    "from skimage.util import random_noise\n",
    "from sklearn.metrics import mean_squared_error as l2\n",
    "\n",
    "base_dir = 'drive/MyDrive/Colab Notebooks/tests/'\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"No tensorboardX package is found. Please install with the command: \\npip install tensorboardX\")\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "7pPbcX2gK7M-",
    "outputId": "86bffee2-e811-4f45-fcf0-18dcdb0b7691"
   },
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "landmarks_frame = pd.read_csv(base_dir + 'dataset/train/face_landmarks.csv')\n",
    "\n",
    "n = 100\n",
    "img_name = landmarks_frame.iloc[n, 0]\n",
    "landmarks = landmarks_frame.iloc[n, 1:].values.astype('float')\n",
    "landmarks = landmarks.reshape(-1, 2)\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "print('First 4 Landmarks: {}'.format(landmarks[:4]))\n",
    "print('Image shape {}'.format( io.imread(os.path.join(base_dir + 'dataset/train/', img_name)).shape  ) )\n",
    "\n",
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='red', cmap='rgb')\n",
    "    plt.pause(0.001)  \n",
    "\n",
    "plt.figure()\n",
    "img = io.imread(os.path.join(base_dir + 'dataset/train/', img_name))\n",
    "show_landmarks(img, landmarks)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FIW4wQchK7NB"
   },
   "source": [
    "def show_landmarks_batch(sample_batch, y_pred=None):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    \"\"\" y_pred: (batch_size, fp_num, 2) \"\"\"\n",
    "    images_batch, landmarks_batch = sample_batch\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    print(images_batch.numpy().shape)\n",
    "    plt.imshow(grid.numpy().transpose(1,0,2).reshape(im_size, -1), cmap=\"gray\")\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n",
    "                    landmarks_batch[i, :, 1].numpy(),\n",
    "                    s=10, marker='.', c='r', label='Real')\n",
    "\n",
    "    if type(y_pred) != type(None):\n",
    "        for i in range(batch_size):\n",
    "            plt.scatter(y_pred[i, :, 0] + i * im_size,\n",
    "                        y_pred[i, :, 1],\n",
    "                        s=10, marker='.', c='b',  label='Prediction')\n",
    "        plt.title('Batch from dataloader')\n",
    "    plt.legend()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a87jqsUm5Xe5"
   },
   "source": [
    "min_arr = np.load( base_dir + 'min_arr.npy' )\n",
    "max_arr = np.load( base_dir + 'max_arr.npy' )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mMLVga_fK7NH"
   },
   "source": [
    "class RandomBlur(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma_range=1.0):\n",
    "        assert isinstance(sigma_range, (float))\n",
    "        self.sigma_range = sigma_range\n",
    "        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "        \n",
    "        sigma = np.random.uniform(0.0, self.sigma_range, 1)[0]\n",
    "        image = filters.gaussian(image, sigma=sigma)\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "\n",
    "class RandomSP(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, amount_range=0.001):\n",
    "        assert isinstance(amount_range, (float))\n",
    "        self.amount_range = amount_range\n",
    "        \n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "        \n",
    "        amount = np.random.uniform(0.0, self.amount_range, 1)[0]\n",
    "        image = random_noise(image, mode='s&p', amount=amount, clip=True)\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "    \n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        \n",
    "        \n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        # shift = (new_h - h) // 2\n",
    "        # img = np.pad(image, shift, mode='constant')\n",
    "        # landmarks = landmarks + [shift, shift]\n",
    "\n",
    "        return img, landmarks\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return image, landmarks\n",
    "    \n",
    "\n",
    "class RandomRotate(object):\n",
    "    # img, facepoints, alpha_degree\n",
    "    \n",
    "    def __init__(self, degree_range):\n",
    "        assert isinstance(degree_range, (int))\n",
    "        assert degree_range >= 1\n",
    "        self.degree_range = degree_range\n",
    "    \n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "        \n",
    "        alpha_degree = np.random.randint(-self.degree_range, self.degree_range, 1)[0]\n",
    "        new_img = transform.rotate(image, alpha_degree, mode='constant') # mode='edge'\n",
    "        \n",
    "        # center = (image.shape[0] / 2 - 0.5, image.shape[1] / 2 - 0.5) # как в skimage.transform.rotate\n",
    "        # center = (image.shape[0] // 2, image.shape[1] // 2)\n",
    "        center = (image.shape[0] / 2, image.shape[1] / 2)\n",
    "        landmarks = np.copy(landmarks)\n",
    "        \n",
    "        vector = landmarks - center\n",
    "        alpha_radian = -np.radians(alpha_degree)\n",
    "        rm = np.array([[np.cos(alpha_radian), -np.sin(alpha_radian)],\n",
    "                      [np.sin(alpha_radian), np.cos(alpha_radian)]], dtype='float32')\n",
    "        \n",
    "        vector = (rm.dot(vector.T)).T\n",
    "        \n",
    "        landmarks = (vector + center).astype(dtype=int)\n",
    "        \n",
    "        return new_img, landmarks\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \n",
    "    def __init__(self, mean=0.4940668, std=0.22909011):\n",
    "        self.mean = mean\n",
    "        self.std = std \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "        #return (image - self.mean) / self.std, landmarks\n",
    "        return (image - min_arr) / (max_arr - min_arr), landmarks\n",
    "    \n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample\n",
    "        return torch.from_numpy(image).type(torch.float), torch.from_numpy(landmarks).type(torch.float)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oVhe9f0TK7NI"
   },
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, device, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:].values.astype('float')\n",
    "        landmarks = landmarks.reshape(-1, 2)\n",
    "\n",
    "        assert len(image.shape) == 2\n",
    "            \n",
    "        if self.transform:\n",
    "            image, landmarks = self.transform((image, landmarks))\n",
    "        \n",
    "        image = image.reshape(1, *image.shape)\n",
    "        landmarks = landmarks.reshape(68 * 2)\n",
    "        sample = image.to(self.device), landmarks.to(self.device)\n",
    "\n",
    "        return sample"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCqGSdFyK7NL",
    "outputId": "4a6e2bd6-75fe-4433-e711-df2bfe2b3d5c"
   },
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PyKNVZojK7NN"
   },
   "source": [
    "train_dataset = FaceLandmarksDataset(csv_file=base_dir + 'dataset/train/face_landmarks.csv',\n",
    "                                     root_dir=base_dir + 'dataset/train',\n",
    "                                     device=device,\n",
    "                                     transform=transforms.Compose([\n",
    "                                                # RandomCrop(80),\n",
    "                                                # Rescale(128), # 192 128\n",
    "                                                # Normalize(),\n",
    "                                                #RandomCrop(128),\n",
    "                                                # RandomRotate(10),\n",
    "                                                # RandomSP(),\n",
    "                                                # RandomBlur(),\n",
    "                                               #RandomCrop(127),\n",
    "                                                ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_dataset = FaceLandmarksDataset(csv_file=base_dir + 'dataset/test/face_landmarks.csv',\n",
    "                                     root_dir=base_dir + 'dataset/test',\n",
    "                                     device=device,\n",
    "                                     transform=transforms.Compose([\n",
    "                                               # Rescale(128),\n",
    "                                               # Normalize(),\n",
    "                                               #RandomCrop(96),\n",
    "                                               ToTensor()\n",
    "                                           ]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8USgVd4nK7NP"
   },
   "source": [
    "def get_data_loaders(train_batch_size=16, val_batch_size=16): # 16\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=val_batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "i0CtYW-BK7NR"
   },
   "source": [
    "class Net(Module):\n",
    "\n",
    "    def __init__(self, input_shape=(1, 96, 96)): # 96\n",
    "        \"\"\" input_shape = (c, h, w)\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cv1 = Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1) \n",
    "        self.ac1 = ReLU()\n",
    "        self.bn1 = BatchNorm2d(32)\n",
    "        self.ap1 = MaxPool2d(kernel_size=2) # 48\n",
    "        \n",
    "        self.cv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) \n",
    "        self.ac2 = ReLU()\n",
    "        self.bn2 = BatchNorm2d(64)\n",
    "        \n",
    "        self.cv3 = Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) \n",
    "        self.ac3 = ReLU()\n",
    "        self.bn3 = BatchNorm2d(64)\n",
    "        self.ap2 = MaxPool2d(2) # 24\n",
    "        \n",
    "        self.cv4 = Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) \n",
    "        self.ac4 = ReLU()\n",
    "        self.bn4 = BatchNorm2d(64)\n",
    "        \n",
    "        self.cv5 = Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) \n",
    "        self.ac5 = ReLU()\n",
    "        self.bn5 = BatchNorm2d(64)\n",
    "        self.ap3 = MaxPool2d(2) # 12\n",
    "        \n",
    "        self.cv6 = Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1) \n",
    "        self.ac6 = ReLU()\n",
    "        self.bn6 = BatchNorm2d(128)\n",
    "        \n",
    "        self.cv7 = Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) \n",
    "        self.ac7 = ReLU()\n",
    "        self.bn7 = BatchNorm2d(128)\n",
    "        self.ap4 = MaxPool2d(2) # 6\n",
    "        \n",
    "        self.cv8 = Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) \n",
    "        self.ac8 = ReLU()\n",
    "        self.bn8 = BatchNorm2d(256)\n",
    "\n",
    "        self.ln1 = Linear(256 * 6 * 6, 1024) # 32 * 6 * 6 # 256 * 8 * 8\n",
    "        self.ac9 = ReLU()\n",
    "        self.bn9 = BatchNorm1d(1024)\n",
    "\n",
    "        self.ln2 = Linear(1024, 68 * 2)\n",
    "\n",
    "        self.layers = [\n",
    "            \n",
    "            self.cv1,\n",
    "            self.ac1,\n",
    "            self.bn1,\n",
    "            self.ap1,\n",
    "\n",
    "            self.cv2,\n",
    "            self.ac2,\n",
    "            self.bn2,\n",
    "\n",
    "            self.cv3,\n",
    "            self.ac3,\n",
    "            self.bn3,\n",
    "            self.ap2,\n",
    "\n",
    "            self.cv4, \n",
    "            self.ac4,\n",
    "            self.bn4,\n",
    "\n",
    "            self.cv5, \n",
    "            self.ac5,\n",
    "            self.bn5,\n",
    "            self.ap3,\n",
    "\n",
    "            self.cv6,\n",
    "            self.ac6,\n",
    "            self.bn6,\n",
    "\n",
    "            self.cv7,\n",
    "            self.ac7,\n",
    "            self.bn7,\n",
    "            self.ap4,\n",
    "\n",
    "            self.cv8,\n",
    "            self.ac8,\n",
    "            self.bn8,\n",
    "            \n",
    "            lambda y: y.view(-1, y.shape[-1] * y.shape[-2] * y.shape[-3]),\n",
    "\n",
    "            self.ln1,\n",
    "            self.ac9,\n",
    "            self.bn9,\n",
    "\n",
    "            self.ln2\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-E-2Uu41K7NT"
   },
   "source": [
    "network = Net()\n",
    "network.to(device)\n",
    "\n",
    "train_dataloader, test_dataloader = get_data_loaders()\n",
    "writer = SummaryWriter(filename_suffix='first', logdir=\"logs\")\n",
    "    \n",
    "criterion = MSELoss()\n",
    "optim = torch.optim.Adam\n",
    "learning_rate = 0.001 # 0.001\n",
    "max_epochs = 10 # 50\n",
    "\n",
    "optimizer = optim(network.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = create_supervised_trainer(network, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    network, metrics={\"L2 loss\": Loss(criterion), \"L1 loss\": Loss(L1Loss())}, device=device\n",
    ")\n",
    "\n",
    "training_history = {'l1':[],'l2':[]}\n",
    "validation_history = {'l1':[],'l2':[]}\n",
    "last_epoch = []"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CKX2xVhtK7NW"
   },
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED(every=10))\n",
    "def log_training_loss(engine):\n",
    "    writer.add_scalar(\"training/loss\", engine.state.output, engine.state.iteration)\n",
    "    writer.flush()\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dataloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    l2_train = metrics[\"L2 loss\"]\n",
    "    l1_train = metrics['L1 loss']\n",
    "    last_epoch.append(0)\n",
    "    training_history['l2'].append(l2_train)\n",
    "    training_history['l1'].append(l1_train)\n",
    "    writer.add_scalar(\"training/epoch_loss\", l2_train, trainer.state.epoch)\n",
    "    \n",
    "    evaluator.run(test_dataloader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    l2 = metrics[\"L2 loss\"]\n",
    "    l1 = metrics['L1 loss']\n",
    "    validation_history['l2'].append(l2)\n",
    "    validation_history['l1'].append(l1)\n",
    "    \n",
    "    writer.add_scalar(\"validation/epoch_loss\", l2, trainer.state.epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    sys.stdout.write(\"\\rTraining/Validation Results - Epoch: {}  Avg MSE: {:.2f} / {:.2f} Avg MAE: {:.2f} / {:.2f}\"\n",
    "          .format(trainer.state.epoch, l2_train, l2, l1_train, l1))\n",
    "   "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRwIZQT8K7NX",
    "outputId": "b38971cd-53b6-4160-b5c8-9d7632812014"
   },
   "source": [
    "trainer.run(train_dataloader, max_epochs=max_epochs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USPV-E29OKqG"
   },
   "source": [
    "### График функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "cTtW_bcdK7NY",
    "outputId": "0199afb4-f8cc-40a0-faa3-9bf20899c4d6"
   },
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "plt.plot(training_history['l2'][1:], label='Train')\n",
    "plt.plot(validation_history['l2'][1:], label='Test')\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.legend(loc=0, fontsize=16)\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_S9DwnjO5f2"
   },
   "source": [
    "### Итоговое качество с помощью функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3pEQ7Ir4Peei"
   },
   "source": [
    "evaluator.run(test_dataloader)\n",
    "metrics = evaluator.state.metrics\n",
    "l2 = metrics[\"L2 loss\"]\n",
    "l1 = metrics['L1 loss']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jA1pRfCrRR4l",
    "outputId": "97de5fdb-f6d4-4753-ecb8-1e6a42c151a1"
   },
   "source": [
    "print('Test metrics:\\nL2 loss: {}\\nL1 loss: {}'.format(l2, l1))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practice task 3, PyTorch Introduction [SOLVED].ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
