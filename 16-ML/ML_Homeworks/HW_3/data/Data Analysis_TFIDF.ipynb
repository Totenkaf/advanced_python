{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "name": "Artem Ustsov_ML-13_HW_3.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqSwzdO_dh3G"
   },
   "source": [
    "# Домашнее задание №3. Дедлайн - 21 ноября\n",
    "Основы машинного обучения. К.Шематоров  \n",
    "Группа ML-13. __Студент - Усцов Артем Алексеевич__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pcuamFGdh3L"
   },
   "source": [
    "## Part 0. Service function declaration\n",
    "\n",
    "Connecting all the libraries necessary for work and declaring functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VndxAD5jdh3M",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531197884,
     "user_tz": -180,
     "elapsed": 3957,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    }
   },
   "source": [
    "# Main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import keras as ks\n",
    "import keras as ks\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "%matplotlib inline"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFm9uvF5duZZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531218175,
     "user_tz": -180,
     "elapsed": 17231,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "fc3d1b27-65c5-4acd-9219-1ccae08c208c"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DbgUwsO7dh3O",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531239258,
     "user_tz": -180,
     "elapsed": 1326,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "14d18d9f-322a-4832-d005-3f75f51b6090"
   },
   "source": [
    "# Train dataset\n",
    "train_df = pd.read_csv(\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv\")\n",
    "train_df.head()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCP0XNv1dh3P",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531248771,
     "user_tz": -180,
     "elapsed": 412,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "3e0d4b27-68b5-49cc-8f66-f47e83a3c20c"
   },
   "source": [
    "# Check the empty data\n",
    "train_df.isnull().any()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "SGsQjwabdh3Q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531251179,
     "user_tz": -180,
     "elapsed": 1303,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "500b09ee-4fcf-4b64-8fe6-84d9acb7a5fe"
   },
   "source": [
    "# Test dataset\n",
    "test_df = pd.read_csv(\"/content/drive/MyDrive/ML_Techno_2021/data/test.csv\")\n",
    "test_df.head()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZP7tXqfqdh3Q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531251182,
     "user_tz": -180,
     "elapsed": 26,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "218d1795-deec-40e7-949d-64ff514de55d"
   },
   "source": [
    "# Check the empty data\n",
    "test_df.isnull().any()"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIVF5lLIdh3R"
   },
   "source": [
    "Как видно, пропусков в данных не имеется. Дополнительная обработка данного случая не требуется"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSVythcedh3R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531252602,
     "user_tz": -180,
     "elapsed": 13,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "0a23210e-7174-4555-85b6-905ec8565fd8"
   },
   "source": [
    "print(f\"Длина вектора данных на обучении - {len(train_df)}\")\n",
    "print(f\"Длина вектора данных на тесте - {len(test_df)}\")\n",
    "print(f\"Соотношение теста к обучающим - {round(len(test_df) / len(train_df), 2)}\")"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du1yP-J2dh3S"
   },
   "source": [
    "Имеем перекос в размере данных на обучающей выборке на 22%"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6zfgXExdh3S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531255224,
     "user_tz": -180,
     "elapsed": 318,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "a157970f-dc12-4fae-e467-4dbd0ba5fc8d"
   },
   "source": [
    "# Labels balance\n",
    "train_df[\"target\"].value_counts()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MzVyfFQedh3T",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531256026,
     "user_tz": -180,
     "elapsed": 9,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    }
   },
   "source": [
    "X_train = train_df[\"title\"].values\n",
    "X_test = test_df[\"title\"].values\n",
    "y_train = train_df[\"target\"].astype(int).values"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeIvkvRrdh3T"
   },
   "source": [
    "## Part 1. Simple baseline realisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GSL-WFipdh3U",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531257313,
     "user_tz": -180,
     "elapsed": 7,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    }
   },
   "source": [
    "y_pred = [int(\"порно\" in text) for text in X_train]"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfelZG4tdh3U",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531257991,
     "user_tz": -180,
     "elapsed": 370,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "b9191858-c1dc-4577-c1a9-ca216d646b27"
   },
   "source": [
    "print(classification_report(y_train, y_pred, digits=3))"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "RW5DgdnBdh3U",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531259225,
     "user_tz": -180,
     "elapsed": 603,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "3b875dbd-2f66-4858-d960-e33a1b0e649b"
   },
   "source": [
    "print(f\"AUC-ROC metric: {round(roc_auc_score(y_train, y_pred), 3)}\")\n",
    "fpr, tpr, _ = roc_curve(y_train, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curve')"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlRQEiQcdh3V"
   },
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YTMQWg2dh3V",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531260222,
     "user_tz": -180,
     "elapsed": 401,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "6f5e510a-1d7b-4a0d-c8e8-f547b7493979"
   },
   "source": [
    "test_df[\"target\"] = [(\"порно\" in text) for text in X_test]\n",
    "\n",
    "# Create file and read in stdout\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"simple_baseline.csv\", index=False)\n",
    "!cat simple_baseline.csv | head"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AN9dzhfdh3W"
   },
   "source": [
    "### Не все так однозначно\n",
    "\n",
    "**не порно**:\n",
    "- Болезни опорно-двигательной системы и импотенция: взаимосвязь\n",
    "- Транссексуальные рыбы - National Geographic Россия: красота мира в каждом кадре\n",
    "- Групповая обзорная экскурсия по Афинам - цена €50\n",
    "- Больного раком Задорнова затравили в соцсетях.\n",
    "- Гомосексуалисты на «Первом канале»? Эрнст и Галкин – скрытая гей-пара российского шоу-бизнеса | Заметки о стиле, моде и жизни\n",
    "\n",
    "**порно**:\n",
    "- Отборная домашка\n",
    "- Сюзанна - карьера горничной / Susanna cameriera perversa (с русским переводом) 1995 г., DVDRip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLX6ln_0dh3W"
   },
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j13wxBUdh3W"
   },
   "source": [
    "Требуется добавить отслеживание \"схожести\" слов, а также важен порядок следования слов в предложении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk0xrnAodh3X"
   },
   "source": [
    "## Part 1. ML baseline realisation\n",
    "Использование базовой векторизации и простейшей модели классификации - мультиномиального наивного байесовского классификатора"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oMmeYAXedh3X",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531264197,
     "user_tz": -180,
     "elapsed": 399,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    }
   },
   "source": [
    "vectorizer = CountVectorizer()\n",
    "model = MultinomialNB()"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mhstD1xkdh3X",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531267000,
     "user_tz": -180,
     "elapsed": 2468,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    }
   },
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zfoosEadh3Y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531267387,
     "user_tz": -180,
     "elapsed": 398,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "fd8162ce-73a8-448d-998f-0e130784e8f1"
   },
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names())"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGaNHWTTdh3Y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531267388,
     "user_tz": -180,
     "elapsed": 14,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "a2d11acb-f103-47a0-857b-8f65e9d74736"
   },
   "source": [
    "id_ = 42\n",
    "print(X_train[id_])\n",
    "x_vector = X_train_vectorized.getrow(id_).toarray()[0]\n",
    "[feature for feature in feature_names[x_vector > 0]]"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOQQjJcedh3Y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531267389,
     "user_tz": -180,
     "elapsed": 10,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "38a6f396-c4af-4961-a6ca-d0ca1a9f3427"
   },
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "y_pred = model.predict(X_train_vectorized)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "806uWvhqdh3Z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531267976,
     "user_tz": -180,
     "elapsed": 593,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "157405ae-8110-4140-ca9b-c2bf265a711c"
   },
   "source": [
    "print(classification_report(y_train, y_pred, digits=3))"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "1-5CJvxIdh3Z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531267977,
     "user_tz": -180,
     "elapsed": 16,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "06f295d4-c6a4-4c46-d7d6-313b03341cd8"
   },
   "source": [
    "print(f\"AUC-ROC metric: {round(roc_auc_score(y_train, y_pred), 3)}\")\n",
    "fpr, tpr, _ = roc_curve(y_train, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curve')"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Flk_hqESdh3a"
   },
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZX50SHD8dh3a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531271088,
     "user_tz": -180,
     "elapsed": 2461,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "f696a6f8-a7b1-4808-cd60-613015d7c946"
   },
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "test_df[\"target\"] = model.predict(X_test_vectorized).astype(bool)\n",
    "\n",
    "# Create file and read in stdout\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"ml_baseline.csv\", index=False)\n",
    "!cat ml_baseline.csv | head"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da4U4tw8dh3b"
   },
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-w30xERdh3b"
   },
   "source": [
    "Как видно, токенизация слов и простейший классификатор хоть и дают результаты лучше, чем при простом отсеивании по ключевому слову, однако качество все равно оставляет желать лучшего. Попробуем увеличить качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_xWNUcIdh3b"
   },
   "source": [
    "## Part 2. Smart data processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSgjY0nYdh3b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531273347,
     "user_tz": -180,
     "elapsed": 316,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "a65f4601-0d33-474f-d5ba-ebcf47c52d03"
   },
   "source": [
    "train_df.info()"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "TnOJv8-cdh3c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637531273636,
     "user_tz": -180,
     "elapsed": 8,
     "user": {
      "displayName": "Артём Усцов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL_P0msl0AGkE2XC5yX6945h5I_5uIPI0sbTqx1g=s64",
      "userId": "16141628694791373514"
     }
    },
    "outputId": "8169e9a8-09d2-43d6-a3ee-9b8ab4bdd03d"
   },
   "source": [
    "# Имеется большой перекос в сторону классификации текста как не порносодержащего\n",
    "train_df.groupby(\"target\").count()"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0L3QLYzdh3d"
   },
   "source": [
    "### Sanitizing input\n",
    "Let's make sure our tweets only have characters we want. We remove '#' characters but keep the words after the '#' sign because they might be relevant (eg: #disaster)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O5VG6c1Sdh3d"
   },
   "source": [
    "input_file = codecs.open(\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv\", \"r\", encoding='utf-8', errors='replace')\n",
    "output_file = open(\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv\", \"w\")\n",
    "\n",
    "def sanitize_characters(raw, clean):    \n",
    "    for line in input_file:\n",
    "        out = line\n",
    "        output_file.write(line)\n",
    "\n",
    "sanitize_characters(input_file, output_file)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mNJYqCTFdh3d",
    "outputId": "4409ecf1-1ca2-400b-d51b-98a48f6a8ac3"
   },
   "source": [
    "# Train dataset\n",
    "train_df = pd.read_csv(\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv/train_clean.csv\")\n",
    "train_df.head(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw7frZILdh3e"
   },
   "source": [
    "Уберем лишние символы в строчках - все возможные запятые, служебные символы и так далее. Также произведем приведение слов к нормальной форме и к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GjFbeQOldh3e"
   },
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "ma = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def clean_text(text, encoding=False):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) #deleting newlines and line-breaks\n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text) #deleting symbols  \n",
    "    text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
    "    text = ' '.join(word for word in text.split() if len(word)>3)\n",
    "    if encoding:\n",
    "        text = text.encode(\"utf-8\")\n",
    "\n",
    "    return text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vmZEjFP3dh3e"
   },
   "source": [
    "# It's too long. DO NOT RUN\n",
    "\n",
    "# train_df['title_clean'] = train_df.apply(lambda x: clean_text(x[u'title']), axis=1)\n",
    "# train_df['title_clean'] = train_df['title_clean'].astype(\"str\")\n",
    "# test_df['title_clean'] = test_df.apply(lambda x: clean_text(x[u'title']), axis=1)\n",
    "# test_df['title_clean'] = test_df['title_clean'].astype(\"str\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jQ55E209dh3f"
   },
   "source": [
    "# train_df.to_csv(\"clean_train_df.csv\")\n",
    "# test_df.to_csv(\"clean_test_df.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1iiWpPoGdh3f"
   },
   "source": [
    "# It's too long. DO NOT RUN\n",
    "\n",
    "# train_df['title_clean'] = train_df.apply(lambda x: clean_text(x[u'title'], True), axis=1)\n",
    "# train_df['title_clean'] = train_df['title_clean'].astype(\"str\")\n",
    "# test_df['title_clean'] = test_df.apply(lambda x: clean_text(x[u'title'], True), axis=1)\n",
    "# test_df['title_clean'] = test_df['title_clean'].astype(\"str\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "veuFQoNrdh3f"
   },
   "source": [
    "# # Будем использовать уже готовые датасеты, так как онлайн обработка производится существенное время\n",
    "clean_train_df = pd.read_csv(\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv/clean_train_df.csv\")\n",
    "clean_train_df['title_clean'] = clean_train_df['title_clean'].astype(\"str\")\n",
    "clean_test_df = pd.read_csv(\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv/clean_test_df.csv\")\n",
    "clean_test_df['title_clean'] = clean_test_df['title_clean'].astype(\"str\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TnpfEWR5dh3g",
    "outputId": "6dc1b2a9-faa7-41bc-ba00-9f1dee778248"
   },
   "source": [
    "clean_train_df.head(50)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T9CiqGh2dh3g"
   },
   "source": [
    "# Как можно видеть, после нормализации и очистки, мы имеем строчки со значением nan - уберем их из датасета\n",
    "clean_train_df = clean_train_df[clean_train_df['title_clean'] != 'nan']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HbvJywMFdh3h"
   },
   "source": [
    "# На первом этапе произведем токенизацию слов\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_train_df[\"tokens\"] = clean_train_df[\"title_clean\"].apply(tokenizer.tokenize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eA5xuIfJdh3h",
    "outputId": "ffc7f8f1-cfa1-4790-b377-274b2467d56a"
   },
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in clean_train_df[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in clean_train_df[\"tokens\"]]\n",
    "\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5C1FidHgdh3h",
    "outputId": "17e7a7c4-8ec4-4a56-90f5-861256661f50"
   },
   "source": [
    "# Посмотрим на распределение длин слов\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMnU4hQqdh3i"
   },
   "source": [
    "## Part 2.1 Tokenizing\n",
    "Используем токенизацию слов на чистом датасете"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "if33u8OYdh3i"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "list_corpus = clean_train_df[\"title_clean\"].tolist()\n",
    "list_labels = clean_train_df[\"target\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.30, \n",
    "                                                                              random_state=40)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvM9A4Wbdh3i"
   },
   "source": [
    "Попробуем спроецировать все наше n-мерное пространство признаков в 2-х мерное пространство при помощи метода главных компонент, чтобы проверить, насколько хорошо разделимы наши данные"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LKk2EQfWdh3i",
    "outputId": "7411c1d3-42ff-45ec-c82a-7bf53f6404e2"
   },
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "def plot_LSA(test_data, test_labels, savepath=\"/content/drive/MyDrive/ML_Techno_2021/data/train.csv/PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2)\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['orange','blue','blue']\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "            red_patch = mpatches.Patch(color='orange', label='Irrelevant')\n",
    "            green_patch = mpatches.Patch(color='blue', label='Porn')\n",
    "            plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTGykFw-dh3j"
   },
   "source": [
    "Под меткой \"Irrelevant\" скрывается все, что не относитеся к порнографии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e1IkXb6dh3j"
   },
   "source": [
    "Обучим модель логистической регресссии и посмотрим на ее качество.\n",
    "Все составляющие модели были ранее подобраны при помощи перебора по сетке. Данный этап здесь опущен, так как занимал достаточно большое время"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E7uf0QaFdh3k"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LOzz6Xuqdh3k",
    "outputId": "7bf4b029-8ca5-4727-b4e7-e3ac5fce27ec"
   },
   "source": [
    "print(classification_report(y_test, y_predicted_counts, digits=3))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-z_S2T1udh3k",
    "outputId": "c426a6a6-012c-4f1c-a5cc-80cfa70c624f"
   },
   "source": [
    "print(f\"AUC-ROC metric: {round(roc_auc_score(y_test, y_predicted_counts), 3)}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_predicted_counts)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curve')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlInxTHjdh3l"
   },
   "source": [
    "Качество заметно улучшилось по сравнению с обычной моделью. Однако все равно есть маневры для улучшения.\n",
    "Посмотрим на то, где наша модель ошибается при помощи confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hi84qxZbdh3l"
   },
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C4Is1UhQdh3m",
    "outputId": "3ff02d4c-0e29-445a-9fb1-b2f71708409b"
   },
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_counts)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Irrelevant','Porn'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Rzy96idh3m"
   },
   "source": [
    "Наша модель склонна к ошибке первого рода - детектирует \"порно\" как \"не порно\". Что с точки зрения пользователя достаточно критично. Посмотрим глубже, как наша модель воспринимает слова и в какую категорию их относит"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G27vlkATdh3m"
   },
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "\n",
    "        classes[class_index] = { 'tops':tops, 'bottom':bottom }\n",
    "\n",
    "    return classes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kve7hy-pdh3n"
   },
   "source": [
    "importance = get_most_important_features(count_vectorizer, clf, 20)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RhhXei_1dh3n"
   },
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Irrelevant', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Porn', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AebUZxmedh3n",
    "outputId": "aceff154-e552-4108-b34b-5a15aa441abb"
   },
   "source": [
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0PVk2Ijdh3o"
   },
   "source": [
    "В целом, модель справляется достаточно хорошо.\n",
    "Однако есть и промахи - \"свинг\" явно относится к категории \"порно\", а \"пора\" наоборот.  \n",
    "Требуется улучшение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i70o0nYidh3o"
   },
   "source": [
    "## Part 2.2. TFIDF\n",
    "Попробуем добавить важность конкретного слова в конкретном описании при помощи tfidf"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0ZV7h8Ndh3o"
   },
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uk1Y2G-ndh3o",
    "outputId": "76858441-692c-4494-f565-6ab9094e50b6"
   },
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_tfidf, y_train)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BecXY6Zxdh3p"
   },
   "source": [
    "Как видно, разделимость классов стала много лучше, однако все равно один класс как бы \"лежит\" внутри другого и для моделей \"классической\" линейной классификации трудно будет их различить"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2XY4kx_tdh3p"
   },
   "source": [
    "param_grid = {\"C\": [1, 5, 10, 30, 40, 50], \"class_weight\" : [\"balanced\"], \n",
    "             \"solver\": [\"newton-cg\"], \"multi_class\" : [\"multinomial\"]}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n-BMkjEvdh3p",
    "outputId": "8e459ab5-2c72-42e2-8e0f-485eee8c7277"
   },
   "source": [
    "estimator = LogisticRegression(n_jobs=-1, random_state=40)\n",
    "clf_tfidf = GridSearchCV(estimator, param_grid, cv = 5)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EvKx3_eNdh3q",
    "outputId": "c161bbde-1122-4d1e-8ab6-9d630e10bb8c"
   },
   "source": [
    "clf_tfidf.best_estimator_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0CZRrigdh3q"
   },
   "source": [
    "Обучим два классификатора - логистическую регрессию и SGD, сравним их качество.\n",
    "Для логистической регрессии параметры были получены ранее при помощи перебора по сетке."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VbXNz1izdh3q"
   },
   "source": [
    "clf_tfidf = LogisticRegression(C=10.0, class_weight='balanced', solver='newton-cg', \n",
    "                               multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TwEUWBjAdh3q",
    "outputId": "7295063a-9591-4f80-fe52-aca4082d36bd"
   },
   "source": [
    "print(classification_report(y_test, y_predicted_tfidf, digits=4))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9W_B40sdh3r"
   },
   "source": [
    "TFIDF позволил выиграть в качестве модели, но не слишком много"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GGKE9lktdh3r"
   },
   "source": [
    "param_grid = {\"alpha\": [0.0001, 0.001, 0.01], \"class_weight\" : [\"balanced\", None], \"eta0\" : [0.1, 0.2, 0.5]}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iufPtH-Udh3r"
   },
   "source": [
    "SGDClassifier?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RUAHIk-zdh3r",
    "outputId": "8383f1e0-caea-4d30-9bb9-d4204480c875"
   },
   "source": [
    "estimator = SGDClassifier(n_jobs=-1, random_state=40, learning_rate = 'adaptive', loss = 'perceptron', early_stopping = True, \n",
    "                          validation_fraction = 0.2, eta0=0.1)\n",
    "clf_tfidf = GridSearchCV(estimator, param_grid, cv = 5)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4S-US6dRdh3s",
    "outputId": "8a9ccb53-ce7d-4aa2-fdc2-ba00c6496237"
   },
   "source": [
    "clf_tfidf.best_estimator_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JnjPHSMSdh3s"
   },
   "source": [
    "clf_tfidf = SGDClassifier(random_state=40,\n",
    "                          learning_rate = 'adaptive', eta0 = 0.1,\n",
    "                          loss = 'perceptron', alpha=0.0001,\n",
    "                          early_stopping = True, validation_fraction = 0.2,\n",
    "                          n_jobs = -1)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predicted_tfidf_sgd = clf_tfidf.predict(X_test_tfidf)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UtiAOrGudh3s",
    "outputId": "88619f70-5d2a-457c-f819-4e8f45e43d5e"
   },
   "source": [
    "print(classification_report(y_test, y_predicted_tfidf_sgd, digits=4))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bphww7Fdh3t"
   },
   "source": [
    "SGD классификатор только ухудшил качество модели"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "auOSKKaadh3t",
    "outputId": "754bbc4d-dd35-412c-be89-0d00b6322d40"
   },
   "source": [
    "print(f\"AUC-ROC metric: {round(roc_auc_score(y_test, y_predicted_tfidf), 3)}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_predicted_tfidf)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curve')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ctA8xEOodh3t",
    "outputId": "6791207f-0ff3-4a48-f90f-e9c60f2d63fa"
   },
   "source": [
    "cm2 = confusion_matrix(y_test, y_predicted_tfidf)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm2, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(\"TFIDF confusion matrix\")\n",
    "print(cm2)\n",
    "print(\"BoW confusion matrix\")\n",
    "print(cm)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KTZLkEcdh3u"
   },
   "source": [
    "Модель стала меньше ошибаться в ошибках I рода.\n",
    "Посмотрим глубже, как наша модель воспринимает слова и в какую категорию их относит"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5MbqeP0Kdh3u"
   },
   "source": [
    "importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 20)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T2cuOMrAdh3u",
    "outputId": "970d0101-a01c-497e-e7eb-ef179f0e2afe"
   },
   "source": [
    "top_scores = [a[0] for a in importance_tfidf[0]['tops']]\n",
    "top_words = [a[1] for a in importance_tfidf[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance_tfidf[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance_tfidf[0]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzsU89qCdh3u"
   },
   "source": [
    "Все равно имеются промахи в словах"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CqaJ9pL0dh3v"
   },
   "source": [
    "X_test_main = test_df[\"title\"].tolist()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_main)\n",
    "\n",
    "test_df[\"target\"] =  clf_tfidf.predict(X_test_tfidf)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "etuLMewfdh3v",
    "outputId": "6d95abcc-5f0f-47f4-8908-115993e5f6ec"
   },
   "source": [
    "# Create file and read in stdout\n",
    "\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"ml_tfidf.csv\", index=False)\n",
    "!cat ml_tfidf.csv | head"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0LzmmW2dh3v"
   },
   "source": [
    "# Conclusion:\n",
    "На Kaggle данная модель показала 0.95466 по f1-метке."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EadnFRQ6dh3v"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
