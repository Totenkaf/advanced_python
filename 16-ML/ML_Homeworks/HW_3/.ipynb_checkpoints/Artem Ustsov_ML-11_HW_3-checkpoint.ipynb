{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3\n",
    "Основы машинного обучения. К.Шематоров  \n",
    "Группа ML-11. __Студент - Усцов Артем Алексеевич__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:24.369290Z",
     "start_time": "2022-04-18T17:58:24.360931Z"
    }
   },
   "source": [
    "# Для функционирования watermark - раскомментируйте строку ниже, либо установите библиотеку в консоли вручную\n",
    "# !pip install watermark\n",
    "%load_ext watermark"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:24.592443Z",
     "start_time": "2022-04-18T17:58:24.556292Z"
    }
   },
   "source": [
    "%watermark -v -m -p numpy,scipy,matplotlib,pandas,sklearn,nltk,codecs -g"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service function declaration\n",
    "\n",
    "Connecting all the libraries necessary for work and declaring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:36:46.465097Z",
     "start_time": "2022-04-20T09:36:46.454086Z"
    }
   },
   "source": [
    "# Main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Метрики\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Отрисовка графики\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Нормализаторы\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "# Обучающие модели\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Статус команд\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Системные службы\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Трансформеры и пайпланы данных\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:36:52.807572Z",
     "start_time": "2022-04-20T09:36:52.800055Z"
    }
   },
   "source": [
    "os.getcwd()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:26.016560Z",
     "start_time": "2022-04-18T17:58:25.535449Z"
    }
   },
   "source": [
    "# Train dataset\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "train_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:26.041403Z",
     "start_time": "2022-04-18T17:58:26.019162Z"
    }
   },
   "source": [
    "# Check the empty data\n",
    "train_df.isnull().any()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:26.604276Z",
     "start_time": "2022-04-18T17:58:26.082484Z"
    }
   },
   "source": [
    "# Test dataset\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "test_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:26.660651Z",
     "start_time": "2022-04-18T17:58:26.640549Z"
    }
   },
   "source": [
    "# Check the empty data\n",
    "test_df.isnull().any()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, пропусков в данных не имеется. Дополнительная обработка данного случая не требуется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:26.690891Z",
     "start_time": "2022-04-18T17:58:26.687337Z"
    }
   },
   "source": [
    "print(f\"Длина вектора данных на обучении - {len(train_df)}\")\n",
    "print(f\"Длина вектора данных на тесте - {len(test_df)}\")\n",
    "print(f\"Соотношение теста к обучающим - {round(len(test_df) / len(train_df), 2)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем перекос в размере данных на обучающей выборке на 22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:26.906670Z",
     "start_time": "2022-04-18T17:58:26.899263Z"
    }
   },
   "source": [
    "list(train_df[\"target\"].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:27.084125Z",
     "start_time": "2022-04-18T17:58:27.077206Z"
    }
   },
   "source": [
    "# Labels balance\n",
    "train_df[\"target\"].value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:27.280056Z",
     "start_time": "2022-04-18T17:58:27.269823Z"
    }
   },
   "source": [
    "false_counts = list(train_df[\"target\"].value_counts())[0]\n",
    "true_counts = list(train_df[\"target\"].value_counts())[1]\n",
    "print(f\"Соотношение меток внутри датасета\")\n",
    "print(f\"Меток класса False от общего количества: {round(false_counts / (false_counts + true_counts), 2) * 100}%\")\n",
    "print(f\"Меток класса True от общего количества: {round(true_counts / (false_counts + true_counts), 2) * 100}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, внутри меток огромный перекос, по-хорошему требуется доразметка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:28.178311Z",
     "start_time": "2022-04-18T17:58:28.174107Z"
    }
   },
   "source": [
    "X_train = train_df[\"title\"].values\n",
    "X_test = test_df[\"title\"].values\n",
    "y_train = train_df[\"target\"].astype(int).values"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple baseline realisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:28.415025Z",
     "start_time": "2022-04-18T17:58:28.387390Z"
    }
   },
   "source": [
    "y_pred = [int(\"порно\" in text) for text in X_train]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:28.855327Z",
     "start_time": "2022-04-18T17:58:28.647759Z"
    }
   },
   "source": [
    "print(classification_report(y_train, y_pred, digits=3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:29.230297Z",
     "start_time": "2022-04-18T17:58:28.906350Z"
    }
   },
   "source": [
    "print(f\"AUC-ROC metric: {round(roc_auc_score(y_train, y_pred), 3)}\")\n",
    "fpr, tpr, _ = roc_curve(y_train, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curve')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:29.608653Z",
     "start_time": "2022-04-18T17:58:29.277260Z"
    }
   },
   "source": [
    "test_df[\"target\"] = [(\"порно\" in text) for text in X_test]\n",
    "\n",
    "# Create file and read in stdout\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"simple_baseline.csv\", index=False)\n",
    "!cat simple_baseline.csv | head"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не все так однозначно\n",
    "\n",
    "**не порно**:\n",
    "- Болезни опорно-двигательной системы и импотенция: взаимосвязь\n",
    "- Транссексуальные рыбы - National Geographic Россия: красота мира в каждом кадре\n",
    "- Групповая обзорная экскурсия по Афинам - цена €50\n",
    "- Больного раком Задорнова затравили в соцсетях.\n",
    "- Гомосексуалисты на «Первом канале»? Эрнст и Галкин – скрытая гей-пара российского шоу-бизнеса | Заметки о стиле, моде и жизни\n",
    "\n",
    "**порно**:\n",
    "- Отборная домашка\n",
    "- Сюзанна - карьера горничной / Susanna cameriera perversa (с русским переводом) 1995 г., DVDRip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуется добавить отслеживание \"схожести\" слов, а также важен порядок следования слов в предложении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML baseline realisation\n",
    "Использование базовой векторизации и простейшей модели классификации - мультиномиального наивного байесовского классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:30.967680Z",
     "start_time": "2022-04-18T17:58:30.965176Z"
    }
   },
   "source": [
    "vectorizer = CountVectorizer()\n",
    "model = MultinomialNB()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:33.320690Z",
     "start_time": "2022-04-18T17:58:31.161375Z"
    }
   },
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:33.669502Z",
     "start_time": "2022-04-18T17:58:33.398664Z"
    }
   },
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:33.732566Z",
     "start_time": "2022-04-18T17:58:33.727029Z"
    }
   },
   "source": [
    "id_ = 42\n",
    "print(X_train[id_])\n",
    "x_vector = X_train_vectorized.getrow(id_).toarray()[0]\n",
    "[feature for feature in feature_names[x_vector > 0]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:33.859836Z",
     "start_time": "2022-04-18T17:58:33.806184Z"
    }
   },
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "y_pred = model.predict(X_train_vectorized)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:34.115803Z",
     "start_time": "2022-04-18T17:58:33.905442Z"
    }
   },
   "source": [
    "print(classification_report(y_train, y_pred, digits=3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:34.337928Z",
     "start_time": "2022-04-18T17:58:34.165914Z"
    }
   },
   "source": [
    "print(f\"AUC-ROC metric: {round(roc_auc_score(y_train, y_pred), 3)}\")\n",
    "fpr, tpr, _ = roc_curve(y_train, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curve')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:37.288433Z",
     "start_time": "2022-04-18T17:58:35.029631Z"
    }
   },
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "test_df[\"target\"] = model.predict(X_test_vectorized).astype(bool)\n",
    "\n",
    "# Create file and read in stdout\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"./data/ml_baseline.csv\", index=False)\n",
    "!cat ml_baseline.csv | head"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, токенизация слов и простейший классификатор хоть и дают результаты лучше, чем при простом отсеивании по ключевому слову, однако качество все равно оставляет желать лучшего. Попробуем увеличить качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data\n",
    "1) __Titles:__ удалялись лишние символы и числа. Производилась лемматизация. Форма слова выбиралась по наиболее лучшим тегам. Далее из полученных списков токенов удалялись стоп слова и слова короче 2 символов. Пустым строкам присваивалось значение '#nan'   \n",
    "\n",
    "2) __Urls:__ сплитились по мусорным символам. Удалялись com ru www. Каждое слово списка билось на ngram-ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:52.859547Z",
     "start_time": "2022-04-18T17:58:52.853156Z"
    }
   },
   "source": [
    "nltk.download('stopwords')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Раскомментируйте, если не установлен пакет\n",
    "# !pip install natasha"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:26:17.501142Z",
     "start_time": "2022-04-20T08:26:17.496380Z"
    },
    "code_folding": []
   },
   "source": [
    "# Полученный результат после токенизатора объединяет в грамотную строчку\n",
    "def joiner(text):\n",
    "    text = ' '.join(word.replace(\"'\",\"\")  for word in text.strip('[]').split(\",\"))\n",
    "    return text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T17:58:55.526597Z",
     "start_time": "2022-04-18T17:58:55.514304Z"
    }
   },
   "source": [
    "# Токенизатор для разбиения слов на слоги\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "tk = SyllableTokenizer()\n",
    "\n",
    "# Разбиение на n-grams\n",
    "def urls_tokenize(df):\n",
    "    stop_ngram = ['ru', 'com', 'www', '', ' ']\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        split = re.split('[^A-Za-z0-9]+', df.url[i])\n",
    "        df.url[i] = [token for token in split if token not in stop_ngram]\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        ngrams =[]\n",
    "        for word in df.url[i]:\n",
    "            ngrams = ngrams + tk.tokenize(word)\n",
    "        df.url[i] = ngrams\n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Вместо Pymorhy была выбрана Natasha после прочтения статьи\n",
    "# https://habr.com/ru/post/516098/\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger, \n",
    "    MorphVocab,\n",
    "    Doc,\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "\n",
    "def preprocessing_data(df):\n",
    "    \n",
    "    def my_filter(text): # к нижнему регистру все буквы\n",
    "        return re.findall(\"[а-яА-ЯёЁa-zA-Z]+\", text.lower())\n",
    "    \n",
    "    print(\"filter start: \")\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        df.title[i] = my_filter(df.title[i]);train_tokenized\n",
    "    print(\"filter finish: \")\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        string = ' '.join(df.title[i])\n",
    "        df.title[i] = string\n",
    "    \n",
    "    def lemmatizer(text):\n",
    "        doc = Doc(text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "        return [_.lemma for _ in doc.tokens]\n",
    "    \n",
    "    print(\"lemmatizer start: \")\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        df.title[i] = lemmatizer(df.title[i])\n",
    "    print(\"lemmatizer finish:\")\n",
    "    \n",
    "    print(\"stop words start\")\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        clear = [token for token in df.title[i] if token not in stopwords.words(['russian', 'english'])]\n",
    "        df.title[i] = clear\n",
    "    print(\"stop words finish\")\n",
    "\n",
    "    print('delete words < 2 start: ')\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        clear = [token for token in df.title[i] if len(token) > 2 ]\n",
    "        df.title[i] = clear\n",
    "    print('delete words < 2 finish: ')\n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что работа по нормализации текста как для titles, так и для urls занимает большое количество времени, поэтому далее процесс обучения опущен. Используются уже обработанные датасеты. Однако при желании есть возможность запустить их и убедиться в работоспособности самостоятельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:45:19.002510Z",
     "start_time": "2022-04-20T08:45:18.601706Z"
    }
   },
   "source": [
    "# Исходный трейновый датасет\n",
    "df = pd.read_csv(\"./data/train.csv\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# # Процессинг titles на трейне\n",
    "# df = preprocessing_data(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# # Процессинг urls на трейне\n",
    "# df = urls_tokenize(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:56:50.943310Z",
     "start_time": "2022-04-20T08:56:50.938721Z"
    }
   },
   "source": [
    "# # Исходный трейновый токенизированный датасет\n",
    "# df_tokenized = pd.read_csv(\"train_tokenized.csv\")\n",
    "# df_tokenized.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:56:46.247928Z",
     "start_time": "2022-04-20T08:56:46.243462Z"
    }
   },
   "source": [
    "# # Развертка токенов в единую строку\n",
    "# df_tokenized[\"title_tokens\"] = df_tokenized.apply(lambda x: joiner(x['title_tokens']), axis=1)\n",
    "# df_tokenized[\"url_tokens\"] = df_tokenized.apply(lambda x: joiner(x['url_tokens']), axis=1)\n",
    "# df_tokenized.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:56:42.766368Z",
     "start_time": "2022-04-20T08:56:42.761866Z"
    }
   },
   "source": [
    "# # Конкатенация датафреймов\n",
    "# df_tokenized[\"title\"] = \"\"\n",
    "# df_tokenized[\"url\"] = \"\"\n",
    "\n",
    "# df_tokenized[\"title\"] = df[\"title\"]\n",
    "# df_tokenized[\"url\"] = df[\"url\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:56:30.972771Z",
     "start_time": "2022-04-20T08:56:30.967969Z"
    }
   },
   "source": [
    "# df_tokenized.to_csv(\".data/train_main.csv\", index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:56:59.076293Z",
     "start_time": "2022-04-20T08:56:58.298393Z"
    }
   },
   "source": [
    "# Исходный трейновый конкатенированный датасет c развернутыми токенами\n",
    "df_main = pd.read_csv(\"./data/train_main.csv\")\n",
    "df_main.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем спроецировать все наше n-мерное пространство признаков в 2-х мерное пространство при помощи метода главных компонент, чтобы проверить, насколько хорошо разделимы наши данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble modelling\n",
    "Во втором файле на гитхаб можно найти подробный анализ tfidfvectorizer, на котором обучена логистическая регрессия. \n",
    "На трейне качество модели было неплохое, однако результаты на Kaggle оставляли желать лучшего - меньше ML baseline\n",
    "\n",
    "Здесь предпринята попытка при помощи TfIdf и CountVectorizer создать ансамбль из моделей логистической регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T19:27:45.279119Z",
     "start_time": "2022-04-18T19:27:45.195073Z"
    }
   },
   "source": [
    "preprocessor1 = ColumnTransformer([\n",
    "    ('url_tokens', TfidfVectorizer(analyzer='word', dtype = np.float32 ), 'url_tokens'),\n",
    "    ('title_tokens', TfidfVectorizer(analyzer='word', dtype = np.float32), 'title_tokens'),   \n",
    "])\n",
    "preprocessor2 = ColumnTransformer([\n",
    "    ('url_tokens', TfidfVectorizer(analyzer='word', dtype = np.float32), 'url_tokens'),\n",
    "    ('title_tokens', TfidfVectorizer(analyzer='word', dtype = np.float32), 'title_tokens'),   \n",
    "])\n",
    "preprocessor3 = ColumnTransformer([\n",
    "    ('url', CountVectorizer(analyzer='char_wb', ngram_range = (2, 4), dtype = np.float32, max_features=100000), 'url'),\n",
    "    ('title', CountVectorizer(analyzer='char_wb',ngram_range = (2, 4), dtype = np.float32, max_features=100000), 'title')  \n",
    "])\n",
    "preprocessor4 = ColumnTransformer([\n",
    "    ('url', CountVectorizer(analyzer = 'char_wb', ngram_range = (2, 4), dtype = np.float32, max_features=100000), 'url'),   \n",
    "    ('title', CountVectorizer(analyzer = 'char_wb', ngram_range = (2, 4), dtype = np.float32, max_features=100000), 'title')\n",
    "])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model1 = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('linear_model', LogisticRegressionCV(class_weight = 'balanced', cv = 3))\n",
    "])\n",
    "model2 = Pipeline([\n",
    "    ('preprocessor', preprocessor2),\n",
    "    ('linear_model', LogisticRegressionCV(class_weight = 'balanced', cv = 3))\n",
    "])\n",
    "model3 = Pipeline([\n",
    "    ('preprocessor', preprocessor3),\n",
    "    ('linear_model', LogisticRegressionCV(class_weight = 'balanced', cv = 3))\n",
    "])\n",
    "model4 = Pipeline([\n",
    "    ('preprocessor', preprocessor4),\n",
    "    ('linear_model', LogisticRegressionCV(class_weight = 'balanced', cv = 3))\n",
    "])\n",
    "\n",
    "\n",
    "ensamble = VotingClassifier(estimators=[('mod1', model1), ('mod2', model2),\n",
    "                                     ('mod3', model3), ('mod4', model4)], voting = 'soft')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:24:26.270378Z",
     "start_time": "2022-04-19T19:24:26.231225Z"
    }
   },
   "source": [
    "df_train, df_test = train_test_split(df_main, train_size=0.8, random_state=42, shuffle=True)\n",
    "y_train = df_train.target.values.astype(np.float32)\n",
    "y_test = df_test.target.values.astype(np.float32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:24:29.476665Z",
     "start_time": "2022-04-19T19:24:29.461221Z"
    }
   },
   "source": [
    "df_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:49:28.398306Z",
     "start_time": "2022-04-19T19:24:35.438663Z"
    }
   },
   "source": [
    "ensamble.fit(df_train, y_train)\n",
    "pred_train = ensamble.predict(df_train)\n",
    "pred_test = ensamble.predict(df_test)\n",
    "print('train f1_score: ', f1_score(pred_train, y_train))\n",
    "print('test f1_score: ', f1_score(pred_test, y_test))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно качество на тесте значительно увеличилось.\n",
    "Перейдем к предсказанию меток на тестовом датасете.   \n",
    "__По аналогии с трейном, тестовый препроцессинг опущен, но им можно воспользоваться при желании__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:22:10.332272Z",
     "start_time": "2022-04-20T09:22:10.324336Z"
    }
   },
   "source": [
    "# В качестве обучающей возьмем все метки трейнового датасета\n",
    "y_train = df_main.target.values.astype(np.float32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:22:11.505199Z",
     "start_time": "2022-04-20T09:22:11.490806Z"
    }
   },
   "source": [
    "order = {\"title_tokens\", \"url_tokens\", \"url\", \"title\"}\n",
    "X = df_main[order]\n",
    "X.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T19:54:27.526385Z",
     "start_time": "2022-04-19T19:54:26.980684Z"
    }
   },
   "source": [
    "df_test = pd.read_csv(\".data/test.csv\")\n",
    "df_test.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T21:27:19.681511Z",
     "start_time": "2022-04-19T19:57:47.887850Z"
    }
   },
   "source": [
    "# df_test = preprocessing_data(df_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:13:17.221112Z",
     "start_time": "2022-04-19T21:27:20.257807Z"
    }
   },
   "source": [
    "# df_test = urls_tokenize(df_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:13:17.801639Z",
     "start_time": "2022-04-20T06:13:17.798334Z"
    }
   },
   "source": [
    "# df_test.rename(columns = {'url' : 'url_tokens', 'title' : 'title_tokens'}, inplace = True)\n",
    "# df_test.to_csv(\"./data/test_tokenized.csv\", index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:13:20.568279Z",
     "start_time": "2022-04-20T06:13:20.119962Z"
    }
   },
   "source": [
    "# df = pd.read_csv(\".data/test.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:13:21.165361Z",
     "start_time": "2022-04-20T06:13:21.160513Z"
    }
   },
   "source": [
    "# df_test[\"url\"] = \"\"\n",
    "# df_test[\"title\"] = \"\"\n",
    "# df_test[\"url\"] = df[\"url\"]\n",
    "# df_test[\"title\"] = df[\"title\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:13:23.909781Z",
     "start_time": "2022-04-20T06:13:22.360270Z"
    }
   },
   "source": [
    "# df_test.to_csv(\"./data/test_main.csv\", index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:30:20.214786Z",
     "start_time": "2022-04-20T09:30:19.276456Z"
    }
   },
   "source": [
    "df_test = pd.read_csv(\"./data/test_main.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:30:24.027568Z",
     "start_time": "2022-04-20T09:30:21.234325Z"
    }
   },
   "source": [
    "df_test[\"title_tokens\"] = df_test.apply(lambda x: joiner(x['title_tokens']), axis=1)\n",
    "df_test[\"url_tokens\"] = df_test.apply(lambda x: joiner(x['url_tokens']), axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:30:25.061805Z",
     "start_time": "2022-04-20T09:30:25.045650Z"
    }
   },
   "source": [
    "X_test = df_test[order]\n",
    "X_test.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:42:19.187759Z",
     "start_time": "2022-04-20T07:13:49.676233Z"
    }
   },
   "source": [
    "ensamble.fit(X, y_train)\n",
    "pred_train = ensamble.predict(X)\n",
    "X_test[\"target\"] = ensamble.predict(X_test)\n",
    "print('train f1_score: ', f1_score(pred_train, y_train))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:43:30.484429Z",
     "start_time": "2022-04-20T07:43:30.476938Z"
    }
   },
   "source": [
    "X_test[\"target\"] = X_test[\"target\"].astype(bool)\n",
    "X_test[\"id\"] = \"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:46:15.286403Z",
     "start_time": "2022-04-20T07:46:15.281233Z"
    }
   },
   "source": [
    "# Добавление id меток\n",
    "df = pd.read_csv(\"./data/test.csv\")\n",
    "X_test[\"id\"] = df[\"id\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:24:34.742957Z",
     "start_time": "2022-04-20T08:24:33.797552Z"
    }
   },
   "source": [
    "# X_test.to_csv(\"./data/test_fpredicted.csv\", index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:31:50.107966Z",
     "start_time": "2022-04-20T09:31:50.101508Z"
    }
   },
   "source": [
    "# X_test = pd.read_csv(\"./data/test_fpredicted.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:47:07.256426Z",
     "start_time": "2022-04-20T07:47:06.885864Z"
    }
   },
   "source": [
    "X_test[[\"id\", \"target\"]].to_csv(\"./data/ml_ensemble.csv\", index=False)\n",
    "!cat ml_tfidf.csv | head"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T09:25:20.533192Z",
     "start_time": "2022-04-20T09:25:20.525178Z"
    }
   },
   "source": [
    "## Conclusion\n",
    "Сабмит показал себя удивительно хорошо.\n",
    "На f1-мере на Kaggle значение составило - 0.98831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "425.191px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "884.861px",
    "left": "1603.33px",
    "right": "20px",
    "top": "147px",
    "width": "429.774px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
